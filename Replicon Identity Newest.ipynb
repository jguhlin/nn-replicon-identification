{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[269132, 1683993, 1953125]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.sparse import csr_matrix\n",
    "from collections import Counter\n",
    "import collections\n",
    "import random\n",
    "from six.moves import urllib\n",
    "from six.moves import xrange  # pylint: disable=redefined-builtin\n",
    "import Bio\n",
    "from Bio import SeqIO\n",
    "import os\n",
    "import concurrent.futures\n",
    "import functools\n",
    "from functools import partial\n",
    "import math\n",
    "import threading\n",
    "import time\n",
    "import random\n",
    "from random import shuffle\n",
    "import pickle\n",
    "import tempfile\n",
    "import ntpath\n",
    "import os.path\n",
    "\n",
    "# k-mer size to use\n",
    "k = 9\n",
    "\n",
    "#\n",
    "# NOTE!!!!!!!!!!!!!!!!\n",
    "#\n",
    "# We can reduce problem space if we get the reverse complement, and add a bit to indicate reversed or not...\n",
    "# Not really.... revcomp just doubles it back up again....\n",
    "#\n",
    "# Also -- Build a recurrent network to predict sequences that come after a given kmer?\n",
    "# Look at word2vec, dna2vec, bag of words, skip-gram\n",
    "#\n",
    "\n",
    "# Problem space\n",
    "space = 5 ** k\n",
    "\n",
    "def partition(n, step, coll):\n",
    "    for i in range(0, len(coll), step):\n",
    "        if (i+n > len(coll)):\n",
    "            break #  raise StopIteration...\n",
    "        yield coll[i:i+n]\n",
    "        \n",
    "def get_kmers(k):\n",
    "    return lambda sequence: partition(k, k, sequence)\n",
    "\n",
    "def convert_nt(c):\n",
    "    return {\"N\": 0, \"A\": 1, \"C\": 2, \"T\": 3, \"G\": 4}.get(c, 0)\n",
    "\n",
    "def convert_nt_complement(c):\n",
    "    return {\"N\": 0, \"A\": 3, \"C\": 4, \"T\": 1, \"G\": 2}.get(c, 0)\n",
    "\n",
    "def convert_kmer_to_int(kmer):\n",
    "    return int(''.join(str(x) for x in (map(convert_nt, kmer))), 5)\n",
    "\n",
    "def convert_kmer_to_int_complement(kmer):\n",
    "    return int(''.join(str(x) for x in reversed(list(map(convert_nt_complement, kmer)))), 5)\n",
    "\n",
    "def convert_base5(n):\n",
    "    return {\"0\": \"N\", \"1\": \"A\", \"2\": \"C\", \"3\": \"T\", \"4\": \"G\"}.get(n,\"N\")\n",
    "\n",
    "def convert_to_kmer(kmer):\n",
    "    return ''.join(map(convert_base5, str(np.base_repr(kmer, 5))))\n",
    "\n",
    "# Not using sparse tensors anymore.\n",
    "   \n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "\n",
    "# Get all kmers, in order, with a sliding window of k (but sliding 1bp for each iteration up to k)\n",
    "# Also get RC for all....\n",
    "\n",
    "def kmer_processor(seq,offset):\n",
    "    return list(map(convert_kmer_to_int, get_kmers(k)(seq[offset:])))\n",
    "\n",
    "def get_kmers_from_seq(sequence):\n",
    "    kmers_from_seq = list()\n",
    "\n",
    "    kp = functools.partial(kmer_processor, sequence)\n",
    "    \n",
    "    for i in map(kp, range(0,k)):\n",
    "        kmers_from_seq.append(i)\n",
    "\n",
    "    rev = sequence[::-1]\n",
    "    kpr = functools.partial(kmer_processor, rev)\n",
    "    \n",
    "    for i in map(kpr, range(0,k)):\n",
    "        kmers_from_seq.append(i)\n",
    "            \n",
    "#    for i in range(0,k):\n",
    "#        kmers_from_seq.append(kmer_processor(sequence,i))\n",
    "#    for i in range(0,k):\n",
    "#        kmers_from_seq.append(kmer_processor(rev, i))\n",
    "    return kmers_from_seq\n",
    "\n",
    "data = list()\n",
    "\n",
    "def load_fasta(filename):\n",
    "    data = dict()\n",
    "    file_base_name = ntpath.basename(filename)\n",
    "    picklefilename = file_base_name + \".picklepickle\"\n",
    "    if os.path.isfile(picklefilename):\n",
    "        print(\"Loading from pickle: \" + filename)\n",
    "        data = pickle.load(open(picklefilename, \"rb\"))\n",
    "    else:\n",
    "        print(\"File not found, generating new sequence: \" + picklefilename)\n",
    "        for seq_record in SeqIO.parse(filename, \"fasta\"):\n",
    "            data.update({seq_record.id:\n",
    "                         get_kmers_from_seq(seq_record.seq.upper())})\n",
    "        pickle.dump(data, open(picklefilename, \"wb\"))\n",
    "    return(data)\n",
    "        \n",
    "def get_kmers_from_file(filename):\n",
    "    kmer_list = list()\n",
    "    for seq_record in SeqIO.parse(filename, \"fasta\"):\n",
    "        kmer_list.extend(get_kmers_from_seq(seq_record.seq.upper()))\n",
    "    return set([item for sublist in kmer_list for item in sublist])\n",
    "\n",
    "all_kmers = set()\n",
    "\n",
    "# Very slow, should make this part concurrent...\n",
    "\n",
    "def find_all_kmers(directory):\n",
    "    kmer_master_list = list()\n",
    "    files = [directory + \"/\" + f for f in os.listdir(directory)]\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=4) as executor:\n",
    "        for i in executor.map(get_kmers_from_file, files):\n",
    "            kmer_master_list.extend(list(i))\n",
    "            kmer_master_list = list(set(kmer_master_list))\n",
    "            print(\"Total unique kmers: \" + str(len(set(kmer_master_list))))\n",
    "    return set(kmer_master_list)\n",
    "\n",
    "def get_categories(directory):\n",
    "    data = list()\n",
    "    files = os.listdir(directory)\n",
    "    for filename in files:\n",
    "        for seq_record in SeqIO.parse(directory + \"/\" + filename, \"fasta\"):\n",
    "            data.append(seq_record.id)\n",
    "    data = sorted(list(set(data)))\n",
    "    return(data)\n",
    "\n",
    "def training_file_generator(directory):\n",
    "    files = [directory + \"/\" + f for f in os.listdir(directory)]\n",
    "    random.shuffle(files)\n",
    "    def gen():\n",
    "        nonlocal files\n",
    "        if (len(files) == 0):\n",
    "            files = [directory + \"/\" + f for f in os.listdir(directory)]\n",
    "            random.shuffle(files)\n",
    "        return(files.pop())\n",
    "    return gen\n",
    "\n",
    "def gen_random_training_data(input_data, window_size):\n",
    "    rname = random.choice(list(input_data.keys()))\n",
    "    rdata = random.choice(input_data[rname])\n",
    "    idx = random.randrange(window_size + 1, len(rdata) - window_size - 1)\n",
    "    tdata = list();\n",
    "    for i in range(idx - window_size - 1, idx + window_size):\n",
    "        if (i < 0): continue\n",
    "        if (i >= len(rdata)): break\n",
    "        if type(rdata[idx]) == list: break;\n",
    "        if type(rdata[i]) == list: break\n",
    "        tdata.append(kmer_dict[rdata[i]])\n",
    "    return tdata, rname\n",
    "\n",
    "# The current state is, each training batch is from a single FASTA file (strain, usually)\n",
    "# This can be ok, as long as training batch is a large number\n",
    "# Need to speed up reading of FASTA files though, maybe pyfaidx or something?\n",
    "\n",
    "# Define the one-hot dictionary...\n",
    "\n",
    "replicons_list = get_categories(\"training-files/\")\n",
    "\n",
    "oh = dict()\n",
    "a = 0\n",
    "for i in replicons_list:\n",
    "    oh[i] = tf.one_hot(a, len(replicons_list))\n",
    "    a += 1\n",
    "    \n",
    "oh = dict()\n",
    "a = 0\n",
    "for i in replicons_list:\n",
    "    oh[i] = a\n",
    "    a += 1\n",
    "    \n",
    "oh = dict()\n",
    "oh['Main'] = [1.0, 0.0, 0.0]\n",
    "oh['pSymA'] = [0.0, 1.0, 0.0]\n",
    "oh['pSymB'] = [0.0, 0.0, 1.0]\n",
    "\n",
    "\n",
    "def generate_training_batch(data, batch_size, window_size):\n",
    "    training_batch_data = list();\n",
    "    while len(training_batch_data) < batch_size:\n",
    "         training_batch_data.append(gen_random_training_data(data, \n",
    "                                                             window_size))\n",
    "    return training_batch_data\n",
    "\n",
    "def train_input_fn():\n",
    "    rdata = generate_training_batch(training_data, 1, window_size)[0]\n",
    "    return rdata[0], oh[rdata[1]]\n",
    "    # return {\"train_input\": rdata[0]}, oh[rdata[1]]\n",
    "\n",
    "batch_size = 1024\n",
    "embedding_size = 128\n",
    "window_size = 7\n",
    "\n",
    "replicons_list = get_categories(\"training-files/\")\n",
    "\n",
    "filegen = training_file_generator(\"training-files/\")\n",
    "\n",
    "repdict = dict()\n",
    "a = 0\n",
    "for i in replicons_list:\n",
    "    repdict[i] = a\n",
    "    a += 1\n",
    "\n",
    "def train_input_fn(data):\n",
    "    tbatch = generate_training_batch(data, 1, window_size)\n",
    "    dat = {'x': tf.convert_to_tensor([tf.convert_to_tensor(get_kmer_embeddings(tbatch[0][0]))])}\n",
    "    lab = tf.convert_to_tensor([repdict[tbatch[0][1]]])\n",
    "    return dat, lab\n",
    "\n",
    "def test_input_fn(data):\n",
    "    tbatch = generate_training_batch(data, 1, window_size)\n",
    "    dat = {'x': tf.convert_to_tensor([tf.convert_to_tensor(get_kmer_embeddings(tbatch[0][0]))])}\n",
    "    lab = tf.convert_to_tensor([repdict[tbatch[0][1]]])\n",
    "    return dat, lab\n",
    "\n",
    "def train_input_fn_raw(data):\n",
    "    tbatch = generate_training_batch(data, 1, window_size)\n",
    "    dat = {'x': (get_kmer_embeddings(tbatch[0][0]))}\n",
    "    lab = [repdict[tbatch[0][1]]]\n",
    "    return dat, lab\n",
    "\n",
    "# Because this was run at work on a smaller sample of files....\n",
    "# with open(\"all_kmers_subset.txt\", \"w\") as f:\n",
    "#     for s in all_kmers:\n",
    "#         f.write(str(s) +\"\\n\")\n",
    "\n",
    "# Because this was run at work on a smaller sample of files....\n",
    "all_kmers = list()\n",
    "# with open(\"all_kmers_subset.txt\", \"r\") as f:\n",
    "#     for line in f:\n",
    "#         all_kmers.append(int(line.strip()))\n",
    "\n",
    "all_kmers = pickle.load(open(\"all_kmers.p\", \"rb\"))\n",
    "\n",
    "all_kmers = set(all_kmers)\n",
    "len(all_kmers)\n",
    "# len(data)\n",
    "\n",
    "# all_kmers = set([item for sublist in data for item in sublist])\n",
    "unused_kmers = set(range(0, space)) - all_kmers\n",
    "\n",
    "kmer_dict = dict()\n",
    "reverse_kmer_dict = dict();\n",
    "\n",
    "a = 0\n",
    "for i in all_kmers:\n",
    "    kmer_dict[i] = a\n",
    "    reverse_kmer_dict[a] = i\n",
    "    a += 1\n",
    "    \n",
    "kmer_count = len(all_kmers)\n",
    "\n",
    "[len(all_kmers), len(unused_kmers), space]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This fn now generates all possible combinations of training data....\n",
    "\n",
    "def gen_training_data(input_data, window_size):\n",
    "    total_data = list()\n",
    "    \n",
    "    for k in input_data.keys():\n",
    "        for kdata in input_data[k]:\n",
    "            for i in range(window_size + 1, len(kdata) - window_size):\n",
    "                kentry = list()\n",
    "                for x in range(i - window_size - 1, i + window_size):\n",
    "                    kentry.append(kmer_dict[kdata[x]])\n",
    "                total_data.append([kentry, k])\n",
    "    return total_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# filegen = training_file_generator(\"training-files/\")\n",
    "# training_data = load_fasta(filegen())\n",
    "# a = gen_training_data(training_data, window_size)\n",
    "# len(a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embeddings = np.load(\"final_embeddings.npy\")\n",
    "\n",
    "def get_kmer_embeddings(kmers):\n",
    "    a = list() # numpy.empty(128 * 15)\n",
    "    for k in kmers:\n",
    "        a.append(embeddings[k])\n",
    "    return np.hstack(a)\n",
    "\n",
    "# training_data = load_fasta(filegen())\n",
    "# get_kmer_embeddings(tbatch[0][0])\n",
    "\n",
    "# tf.convert_to_tensor([tf.convert_to_tensor(get_kmer_embeddings(tbatch[0][0]))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "WARNING:tensorflow:Using temporary folder as model directory: C:\\Users\\Joey\\AppData\\Local\\Temp\\tmpyxwxw13w\n",
      "INFO:tensorflow:Using config: {'_model_dir': 'C:\\\\Users\\\\Joey\\\\AppData\\\\Local\\\\Temp\\\\tmpyxwxw13w', '_tf_random_seed': 1, '_save_summary_steps': 100, '_save_checkpoints_secs': 600, '_save_checkpoints_steps': None, '_session_config': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100}\n"
     ]
    }
   ],
   "source": [
    "feature_columns = [tf.feature_column.numeric_column(\"x\", shape=[1920])]\n",
    "nn = tf.estimator.DNNClassifier(feature_columns=feature_columns,\n",
    "                                hidden_units = [5000,1000,100,50],\n",
    "                                activation_fn=tf.nn.relu,\n",
    "                                dropout=0.2,\n",
    "                                n_classes=len(replicons_list),\n",
    "                                optimizer=\"Adam\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading from pickle: training-files//SM11.final.fasta\n",
      "Loading from pickle: training-files//KH35c.final.fasta\n",
      "Loading from pickle: training-files//USDA1157.final.fasta\n"
     ]
    }
   ],
   "source": [
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "\n",
    "filegen = training_file_generator(\"training-files/\")\n",
    "\n",
    "for i in xrange(10):\n",
    "    training_data = load_fasta(filegen())\n",
    "    tfn = functools.partial(train_input_fn, training_data)\n",
    "    nn.train(input_fn=tfn, steps=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in xrange(10):\n",
    "    filegen = training_file_generator(\"training-files/\")\n",
    "    training_data = load_fasta(filegen())\n",
    "    tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "    tfn = functools.partial(train_input_fn, training_data)\n",
    "    accuracy_score = nn.evaluate(input_fn=tfn, steps=10000)\n",
    "    print(\"\\nTest Accuracy: {0:f}\\n\".format(accuracy_score['accuracy']))\n",
    "    print(\"Average Loss: {0:f}\".format(accuracy_score['average_loss']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'x': array([-0.01598681,  0.08780892,  0.05043568, ...,  0.03577497,\n",
       "          0.03044195,  0.04390863], dtype=float32)}, [2])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_input_fn_raw(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Emel_M2_Main',\n",
       " 'Emel_M2_Uni1',\n",
       " 'Emel_M2_Uni2',\n",
       " 'Emel_M2_Uni3',\n",
       " 'Emel_WSM419_Main',\n",
       " 'Emel_WSM419_Uni1',\n",
       " 'Emel_WSM419_Uni11',\n",
       " 'Emel_WSM419_Uni2',\n",
       " 'HM006_Accessory_A',\n",
       " 'KH35c_Accessory_A',\n",
       " 'M162_Accessory_A',\n",
       " 'M270_Accessory_A',\n",
       " 'M270_Accessory_B',\n",
       " 'M270_Accessory_C',\n",
       " 'Main',\n",
       " 'Rm41_Accessory_A',\n",
       " 'T073_Accessory_A',\n",
       " 'USDA1021_Accessory_A',\n",
       " 'USDA1157_Accessory_A',\n",
       " 'pHRB800',\n",
       " 'pHRC017',\n",
       " 'pRmeGR4a',\n",
       " 'pRmeGR4b',\n",
       " 'pSINME01',\n",
       " 'pSINME02',\n",
       " 'pSMED03_WSM419',\n",
       " 'pSymA',\n",
       " 'pSymB']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "replicons_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
