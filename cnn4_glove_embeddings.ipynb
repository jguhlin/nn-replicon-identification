{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "k = 9\n",
    "\n",
    "def convert_base5(n):\n",
    "    return {\"0\": \"N\", \"1\": \"A\", \"2\": \"C\", \"3\": \"T\", \"4\": \"G\"}.get(n,\"N\")\n",
    "\n",
    "def convert_nt(c):\n",
    "    return {\"N\": 0, \"A\": 1, \"C\": 2, \"T\": 3, \"G\": 4}.get(c, 0)\n",
    "\n",
    "def convert_nt_complement(c):\n",
    "    return {\"N\": 0, \"A\": 3, \"C\": 4, \"T\": 1, \"G\": 2}.get(c, 0)\n",
    "\n",
    "def convert_kmer_to_int(kmer):\n",
    "    return int(''.join(str(x) for x in (map(convert_nt, kmer))), 5)\n",
    "\n",
    "def kmer_processor(seq,offset):\n",
    "    return list(map(convert_kmer_to_int, get_kmers(k)(seq[offset:])))\n",
    "\n",
    "def partition(n, step, coll):\n",
    "    for i in range(0, len(coll), step):\n",
    "        if (i+n > len(coll)):\n",
    "            break #  raise StopIteration...\n",
    "        yield coll[i:i+n]\n",
    "\n",
    "def get_kmers(k):\n",
    "    return lambda sequence: partition(k, k, sequence)\n",
    "\n",
    "def get_kmers_from_seq(sequence):\n",
    "    kmers_from_seq = list()\n",
    "\n",
    "    kp = functools.partial(kmer_processor, sequence)\n",
    "    \n",
    "    for i in map(kp, range(0,k)):\n",
    "        kmers_from_seq.append(i)\n",
    "\n",
    "    rev = sequence[::-1]\n",
    "    kpr = functools.partial(kmer_processor, rev)\n",
    "    \n",
    "    for i in map(kpr, range(0,k)):\n",
    "        kmers_from_seq.append(i)\n",
    "            \n",
    "#    for i in range(0,k):\n",
    "#        kmers_from_seq.append(kmer_processor(sequence,i))\n",
    "#    for i in range(0,k):\n",
    "#        kmers_from_seq.append(kmer_processor(rev, i))\n",
    "    return kmers_from_seq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"Glove/vocab.txt\", 'r') as f:\n",
    "    words = [x.rstrip().split(' ')[0] for x in f.readlines()]\n",
    "\n",
    "with open(\"Glove/vectors.txt\", 'r') as f:\n",
    "    vectors = {}\n",
    "    for line in f:\n",
    "        vals = line.rstrip().split(' ')\n",
    "        vectors[vals[0]] = [float(x) for x in vals[1:]]\n",
    "\n",
    "vocab_size = len(words)\n",
    "vocab = {w: idx for idx, w in enumerate(words)}\n",
    "ivocab = {idx: w for idx, w in enumerate(words)}\n",
    "\n",
    "vector_dim = len(vectors[ivocab[0]])\n",
    "W = np.zeros((vocab_size, vector_dim))\n",
    "for word, v in vectors.items():\n",
    "    if word == '<unk>':\n",
    "        continue\n",
    "    W[vocab[word], :] = v\n",
    "\n",
    "# normalize each word vector to unit variance\n",
    "W_norm = np.zeros(W.shape)\n",
    "d = (np.sum(W ** 2, 1) ** (0.5))\n",
    "W_norm = (W.T / d).T\n",
    "\n",
    "def convert_to_kmer(kmer):\n",
    "    return ''.join(map(convert_base5, str(np.base_repr(kmer, 5))))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab['1008064']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance(W, vocab, ivocab, input_term):\n",
    "    for idx, term in enumerate(input_term.split(' ')):\n",
    "        if term in vocab:\n",
    "            print('Word: %s  Position in vocabulary: %i' % (convert_to_kmer(int(term)), vocab[term]))\n",
    "            if idx == 0:\n",
    "                vec_result = np.copy(W[vocab[term], :])\n",
    "            else:\n",
    "                vec_result += W[vocab[term], :] \n",
    "        else:\n",
    "            print('Word: %s  Out of dictionary!\\n' % (convert_to_kmer(int(term))))\n",
    "            return\n",
    "    \n",
    "    vec_norm = np.zeros(vec_result.shape)\n",
    "    d = (np.sum(vec_result ** 2,) ** (0.5))\n",
    "    vec_norm = (vec_result.T / d).T\n",
    "\n",
    "    dist = np.dot(W, vec_norm.T)\n",
    "\n",
    "    for term in input_term.split(' '):\n",
    "        index = vocab[term]\n",
    "        dist[index] = -np.Inf\n",
    "\n",
    "    a = np.argsort(-dist)[:100]\n",
    "\n",
    "    print(\"\\n                               Word       Cosine distance\\n\")\n",
    "    print(\"---------------------------------------------------------\\n\")\n",
    "    for x in a:\n",
    "        print(\"%35s\\t\\t%f\\n\" % (convert_to_kmer(int(ivocab[x])), dist[x]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word: ATGACGATC  Position in vocabulary: 16524\n",
      "\n",
      "                               Word       Cosine distance\n",
      "\n",
      "---------------------------------------------------------\n",
      "\n",
      "                          GCGGCGATC\t\t0.321689\n",
      "\n",
      "                          CGGATCTCG\t\t0.301249\n",
      "\n",
      "                          TGATATTTG\t\t0.298273\n",
      "\n",
      "                          TCGACGATG\t\t0.296905\n",
      "\n",
      "                          AGCACGATC\t\t0.294950\n",
      "\n",
      "                          ATGATCGCC\t\t0.291062\n",
      "\n",
      "                          TATTCCTTG\t\t0.283953\n",
      "\n",
      "                          GCGATCTCG\t\t0.280744\n",
      "\n",
      "                          AGCGAACTG\t\t0.280360\n",
      "\n",
      "                          GCAAGCGCC\t\t0.280148\n",
      "\n",
      "                          TACGCNNNN\t\t0.273024\n",
      "\n",
      "                          TCGCCGATC\t\t0.271661\n",
      "\n",
      "                          GCGATCGCC\t\t0.270788\n",
      "\n",
      "                          CTGACGCTG\t\t0.270154\n",
      "\n",
      "                          ACATCGGTC\t\t0.267977\n",
      "\n",
      "                          CCGAGCGTG\t\t0.267093\n",
      "\n",
      "                          CCGTCGATC\t\t0.266004\n",
      "\n",
      "                          CCGATCCCG\t\t0.265412\n",
      "\n",
      "                          CTGTCGATC\t\t0.265294\n",
      "\n",
      "                          CGGTTGCGG\t\t0.264336\n",
      "\n",
      "                          ACGTAGTCG\t\t0.264066\n",
      "\n",
      "                          ATCCTGATT\t\t0.263745\n",
      "\n",
      "                          CCGGTGACG\t\t0.263546\n",
      "\n",
      "                          CCGGTGATC\t\t0.260371\n",
      "\n",
      "                          TTCGACACG\t\t0.260114\n",
      "\n",
      "                          ACGAAGGCG\t\t0.259242\n",
      "\n",
      "                          GCGATGATC\t\t0.258910\n",
      "\n",
      "                          GTGTTCAAC\t\t0.254153\n",
      "\n",
      "                          TGGTCGATA\t\t0.252265\n",
      "\n",
      "                          AATACTCTG\t\t0.252192\n",
      "\n",
      "                          TCGACCGCC\t\t0.251661\n",
      "\n",
      "                          AGCGCCATG\t\t0.251451\n",
      "\n",
      "                          ATGTTGGCC\t\t0.250913\n",
      "\n",
      "                          ACGAAGCGC\t\t0.250768\n",
      "\n",
      "                          CCAAGTTCC\t\t0.250355\n",
      "\n",
      "                          GCCCCCTCT\t\t0.250168\n",
      "\n",
      "                          TTGATGCCG\t\t0.250042\n",
      "\n",
      "                          GTCGTCTTC\t\t0.247122\n",
      "\n",
      "                          ATCAGCGCC\t\t0.246919\n",
      "\n",
      "                          TTGCCGACG\t\t0.245994\n",
      "\n",
      "                          GCCGTCATG\t\t0.245988\n",
      "\n",
      "                          TGGGTCGTG\t\t0.245839\n",
      "\n",
      "                          ATCGCGGCG\t\t0.245816\n",
      "\n",
      "                          GAGGCGACG\t\t0.244959\n",
      "\n",
      "                          TCGTCGACG\t\t0.244772\n",
      "\n",
      "                          CCGATCCAG\t\t0.244695\n",
      "\n",
      "                          GTGAAGGCG\t\t0.243684\n",
      "\n",
      "                          ACGCCGACG\t\t0.242642\n",
      "\n",
      "                          TCGATCGCC\t\t0.242169\n",
      "\n",
      "                          AATCATCTC\t\t0.241508\n",
      "\n",
      "                          GCGAGGCTG\t\t0.241098\n",
      "\n",
      "                          ACGACGAAG\t\t0.241073\n",
      "\n",
      "                          GCGATCTGC\t\t0.241031\n",
      "\n",
      "                          AGGATCAGC\t\t0.240995\n",
      "\n",
      "                          CCGCCGAGC\t\t0.238985\n",
      "\n",
      "                          TGCAGGATG\t\t0.238883\n",
      "\n",
      "                          CCCGCCTTC\t\t0.237990\n",
      "\n",
      "                          AGAATCACG\t\t0.236862\n",
      "\n",
      "                          TTTATGCTT\t\t0.235939\n",
      "\n",
      "                          CCCTTGATG\t\t0.235911\n",
      "\n",
      "                          GAGACGACC\t\t0.235184\n",
      "\n",
      "                          ACCGCATTC\t\t0.234477\n",
      "\n",
      "                          TCGCCGATG\t\t0.233412\n",
      "\n",
      "                          TCGAGCAGC\t\t0.233153\n",
      "\n",
      "                          AGGACGGTG\t\t0.232392\n",
      "\n",
      "                          GGCCCGTAG\t\t0.232218\n",
      "\n",
      "                          GGCATCATG\t\t0.231430\n",
      "\n",
      "                          TAGATACCA\t\t0.231242\n",
      "\n",
      "                          CCGATCCCC\t\t0.230162\n",
      "\n",
      "                          TTCTCGCCC\t\t0.230014\n",
      "\n",
      "                          ATCAGCACG\t\t0.229643\n",
      "\n",
      "                          TCGGAACCG\t\t0.229494\n",
      "\n",
      "                          GCGATGGCA\t\t0.229354\n",
      "\n",
      "                          ATCTGGCGG\t\t0.228955\n",
      "\n",
      "                          AGGATACCT\t\t0.228740\n",
      "\n",
      "                          AAGGGGAGC\t\t0.228528\n",
      "\n",
      "                          TCGATCGAG\t\t0.227877\n",
      "\n",
      "                          ACGATGATG\t\t0.227805\n",
      "\n",
      "                          CCGGTTTTC\t\t0.227459\n",
      "\n",
      "                          TCGAAGGCG\t\t0.227316\n",
      "\n",
      "                          TCATGTTCG\t\t0.227158\n",
      "\n",
      "                          AAGGGTGAC\t\t0.226832\n",
      "\n",
      "                          TGGTTGAAG\t\t0.226713\n",
      "\n",
      "                          ATCGCCTCC\t\t0.226600\n",
      "\n",
      "                          ACCTGGAAG\t\t0.226356\n",
      "\n",
      "                          ATGCGGAAG\t\t0.226309\n",
      "\n",
      "                          GCCTCGTCG\t\t0.225796\n",
      "\n",
      "                          TCGATCGAC\t\t0.225600\n",
      "\n",
      "                          CACGACATG\t\t0.225557\n",
      "\n",
      "                          GCCGCGATG\t\t0.225314\n",
      "\n",
      "                          GGCGCGATC\t\t0.225286\n",
      "\n",
      "                          AGCTATTAC\t\t0.225038\n",
      "\n",
      "                          GAAATAACG\t\t0.224890\n",
      "\n",
      "                          CCCATGACG\t\t0.224645\n",
      "\n",
      "                          GGGTTGATC\t\t0.224633\n",
      "\n",
      "                          TTGACGGCA\t\t0.224590\n",
      "\n",
      "                          TCGATCCTC\t\t0.224558\n",
      "\n",
      "                          TCGTAATTC\t\t0.224546\n",
      "\n",
      "                          ATCACCGGC\t\t0.224476\n",
      "\n",
      "                          AAGGCGAAG\t\t0.224455\n",
      "\n"
     ]
    }
   ],
   "source": [
    "distance(W_norm, vocab, ivocab, str(convert_kmer_to_int(\"ATGACGATC\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.save(\"glove_embeddings.np\", W_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'CCGCCGCCG'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convert_to_kmer(1008064)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GCGGCGATC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tensorflow Model\n",
    "\n",
    "# Vector length is 256\n",
    "\n",
    "# Input is 15 kmers (can be altered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import functools\n",
    "from functools import partial\n",
    "import os.path\n",
    "import Bio\n",
    "from Bio import SeqIO\n",
    "import random\n",
    "from random import shuffle\n",
    "import ntpath\n",
    "import pickle\n",
    "import sys\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 256\n",
    "\n",
    "# sess = tf.Session()\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "# Weights = tf.Variable(tf.constant(0.0, shape=[vocab_size, embedding_dim]), trainable=False, name=\"Weights\")\n",
    "\n",
    "# embedding_placeholder = tf.placeholder(tf.float32, [vocab_size, embedding_dim])\n",
    "# embedding_init = Weights.assign(embedding_placeholder)\n",
    "\n",
    "# sess.run(embedding_init, feed_dict={embedding_placeholder: W_norm})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_fasta(filename):\n",
    "    # tf.summary.text(\"File\", tf.as_string(filename))\n",
    "    data = dict()\n",
    "    file_base_name = ntpath.basename(filename)\n",
    "    picklefilename = file_base_name + \".picklepickle\"\n",
    "    if os.path.isfile(picklefilename):\n",
    "        print(\"Loading from pickle: \" + filename)\n",
    "        data = pickle.load(open(picklefilename, \"rb\"))\n",
    "    else:\n",
    "        print(\"File not found, generating new sequence: \" + picklefilename)\n",
    "        for seq_record in SeqIO.parse(filename, \"fasta\"):\n",
    "            data.update({seq_record.id:\n",
    "                         get_kmers_from_seq(seq_record.seq.upper())})\n",
    "        pickle.dump(data, open(picklefilename, \"wb\"))\n",
    "    sys.stdout.flush()\n",
    "    return(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_training_data_generator(input_data, window_size, repdict):\n",
    "    for k in input_data.keys():\n",
    "        for kdata in input_data[k]:\n",
    "            for i in range(window_size + 1, len(kdata) - window_size):\n",
    "                kentry = list()\n",
    "                for x in range(i - window_size - 1, i + window_size):\n",
    "                    kentry.append(vocab[str(kdata[x])])\n",
    "                yield(kentry, [repdict[k]])\n",
    "\n",
    "\n",
    "def get_categories(directory):\n",
    "    data = list()\n",
    "    files = os.listdir(directory)\n",
    "    for filename in files:\n",
    "        for seq_record in SeqIO.parse(directory + \"/\" + filename, \"fasta\"):\n",
    "            data.append(seq_record.id)\n",
    "    data = sorted(list(set(data)))\n",
    "    return(data)\n",
    "\n",
    "replicons_list = get_categories(\"training-files/\")\n",
    "\n",
    "def kmer_generator(directory, window_size):\n",
    "    files = [directory + \"/\" + f for f in os.listdir(directory)]\n",
    "    random.shuffle(files)\n",
    "    \n",
    "    replicons_list = get_categories(\"training-files/\")\n",
    "    repdict = dict()\n",
    "    a = 0\n",
    "    for i in replicons_list:\n",
    "        repdict[i] = a\n",
    "        a += 1\n",
    "    \n",
    "    for f in files:\n",
    "        yield from gen_training_data_generator(load_fasta(f), window_size, repdict)\n",
    "\n",
    "\n",
    "def input_fn():\n",
    "    kmer_gen = functools.partial(kmer_generator, \"training-files/\", 7)\n",
    "\n",
    "    ds = tf.data.Dataset.from_generator(kmer_gen, \n",
    "                                        (tf.float32,\n",
    "                                         tf.int64),\n",
    "                                        (tf.TensorShape([15]),\n",
    "                                         tf.TensorShape(None)))\n",
    "                                        \n",
    "#    # Numbers reduced to run on my desktop\n",
    "#    ds = ds.repeat(5)\n",
    "#    ds = ds.prefetch(5000) # Each batch is only 2048, so prefetch 5000\n",
    "#    ds = ds.shuffle(buffer_size=1000000) # Large buffer size for better randomization\n",
    "#    ds = ds.batch(2048) # Reduced from 5000 so it runs quicker\n",
    "    \n",
    "#    ds = ds.repeat(1)\n",
    "#    ds = ds.prefetch(2)\n",
    "#    ds = ds.shuffle(buffer_size=500)\n",
    "    ds = ds.batch(20)\n",
    "    \n",
    "    def add_labels(arr, lab):\n",
    "        return({\"kmers\": arr}, lab)\n",
    "    \n",
    "    ds = ds.map(add_labels)\n",
    "    iterator = ds.make_one_shot_iterator()\n",
    "    batch_features, batch_labels = iterator.get_next()\n",
    "    return batch_features, batch_labels\n",
    "\n",
    "def init():\n",
    "    return W\n",
    "\n",
    "replicons_fc = tf.feature_column.categorical_column_with_vocabulary_list(\n",
    "    key='label',\n",
    "    vocabulary_list=replicons_list)\n",
    "    \n",
    "kmers_fc = tf.feature_column.numeric_column(key=\"kmers\", shape=15, dtype=tf.int64)\n",
    "# kmers_dict = tf.feature_column.categorical_column_with_vocabulary_list(\n",
    "#     key=\"kmers\",\n",
    "#     shape=15,\n",
    "#     vocabulary_list=vocab.keys())\n",
    "# kmers_fc_embed = tf.feature_column.embedding_column(\n",
    "#     categorical_column=kmers_dict, \n",
    "#     dimension=256,\n",
    "#     initializer=init,\n",
    "#     trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading from pickle: training-files//WSM419.final.fasta\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([21851,\n",
       "  103912,\n",
       "  5944,\n",
       "  155395,\n",
       "  164903,\n",
       "  18374,\n",
       "  1951,\n",
       "  234160,\n",
       "  137275,\n",
       "  79405,\n",
       "  36675,\n",
       "  153357,\n",
       "  82637,\n",
       "  25877,\n",
       "  125731],\n",
       " [5])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kmer_gen = functools.partial(kmer_generator, \"training-files/\", 7)\n",
    "next(kmer_gen())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File not found, generating new sequence: SM11.final.fasta.picklepickle\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 85790., 166485.,  43233., 101577., 154642., 188957.,  77993.,\n",
       "        126535., 107128.,  13017., 121746.,   1037., 102026., 128846.,\n",
       "        149349.]], dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sess = tf.InteractiveSession()\n",
    "v = input_fn()\n",
    "#v[0][\"kmers\"].eval()\n",
    "a = tf.feature_column.input_layer(v[0], [kmers_fc])\n",
    "a = tf.Print(a, [a], message=\"This is a: \")\n",
    "a.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[166485,  43233, 101577, 154642, 188957,  77993, 126535, 107128,\n",
       "         13017, 121746,   1037, 102026, 128846, 149349,  95425]],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#v = input_fn()\n",
    "# v[0][\"kmers\"]\n",
    "a = tf.feature_column.input_layer(v[0], kmers_fc)\n",
    "a = tf.cast(a, tf.int64)\n",
    "a.eval()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[-0.01377905, -0.0105249 , -0.04465437, ...,  0.00205964,\n",
       "         -0.05210606,  0.00690691],\n",
       "        [ 0.10313533,  0.02833033, -0.04824345, ..., -0.01655637,\n",
       "         -0.07461011,  0.07710814],\n",
       "        [ 0.07772083,  0.00162455, -0.06415386, ..., -0.02984827,\n",
       "         -0.04164281,  0.05927063],\n",
       "        ...,\n",
       "        [ 0.00598741,  0.00918654,  0.02112901, ..., -0.00552926,\n",
       "          0.07925026, -0.00039346],\n",
       "        [ 0.01271917,  0.00480851,  0.04154439, ..., -0.01210083,\n",
       "         -0.02720928, -0.05585257],\n",
       "        [-0.01477062, -0.01350971,  0.11504258, ..., -0.01094713,\n",
       "          0.12314176,  0.01675047]]], dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ks = tf.feature_column.input_layer(v[0], [kmers_fc])\n",
    "\n",
    "embedding_dim = 256\n",
    "\n",
    "Weights = tf.Variable(tf.constant(0.0, shape=[vocab_size, embedding_dim]), trainable=False, name=\"Weights\")\n",
    "\n",
    "embedding_placeholder = tf.placeholder(tf.float32, [vocab_size, embedding_dim])\n",
    "embedding_init = Weights.assign(embedding_placeholder)\n",
    "\n",
    "sess.run(embedding_init, feed_dict={embedding_placeholder: W_norm})\n",
    "\n",
    "a = tf.feature_column.input_layer(v[0], kmers_fc)\n",
    "a = tf.cast(a, tf.int64)\n",
    "words = tf.nn.embedding_lookup(Weights, a)\n",
    "\n",
    "b = tf.Print(words, [words])\n",
    "\n",
    "b.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  1,  15, 256])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.shape(b).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_model_fn(features, labels, mode):\n",
    "    \n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        tf.logging.info(\"my_model_fn: PREDICT, {}\".format(mode))\n",
    "    elif mode == tf.estimator.ModeKeys.EVAL:\n",
    "        tf.logging.info(\"my_model_fn: EVAL, {}\".format(mode))\n",
    "    elif mode == tf.estimator.ModeKeys.TRAIN:\n",
    "        tf.logging.info(\"my_model_fn: TRAIN, {}\".format(mode))\n",
    "\n",
    "    Weights = tf.Variable(tf.constant(0.0, shape=[vocab_size, embedding_dim]), trainable=False, name=\"Weights\")\n",
    "    embedding_placeholder = tf.placeholder(tf.float32, [vocab_size, embedding_dim])\n",
    "    embedding_init = Weights.assign(embedding_placeholder)\n",
    "    \n",
    "    def init_fn(scaffold, sess):\n",
    "        sess.run(Weights.initializer, {Weights.initial_value: W})\n",
    "    scaffold = tf.train.Scaffold(init_fn=init_fn)\n",
    "    \n",
    "    inputs = tf.feature_column.input_layer(features, [kmers_fc])\n",
    "    input_i64 = tf.cast(inputs, tf.int64)\n",
    "    embedded_kmers = tf.nn.embedding_lookup(Weights, input_i64)\n",
    "    \n",
    "    # input_layer = tf.reshape(embedded_kmers, [-1, 15, 256 ,1])\n",
    "    input_layer = tf.reshape(embedded_kmers, [-1, 3840])\n",
    "    input_layer = tf.cast(input_layer, tf.float32)\n",
    "    \n",
    "#    conv1 = tf.layers.conv2d(inputs = input_layer,\n",
    "#                             filters=32,\n",
    "#                             kernel_size=[-1,2,256],\n",
    "#                             strides=3,\n",
    "#                             padding=\"same\",\n",
    "#                             name=\"Conv1\",\n",
    "#                             activation=None)\n",
    "    \n",
    "#    avg_pool1 = tf.layers.average_pooling2d(conv1, \n",
    "#                                            pool_size=[-1,4,32], \n",
    "#                                            strides=[-1,2,16],\n",
    "#                                            padding=\"same\",\n",
    "#                                            name=\"AvgPooling_1\")\n",
    "    \n",
    "    # 29 is number of replicons\n",
    "    \n",
    "#    print(tf.shape(avg_pool1))\n",
    "    \n",
    "    #logits = tf.layers.dense(units=len(replicons_list), inputs=avg_pool1)\n",
    "    # inputs=avg_pool1, units=len(replicons_list))\n",
    "    \n",
    "    h1 = tf.layers.Dense(1000, activation=tf.nn.relu)(input_layer)\n",
    "    h2 = tf.layers.Dense(500, activation=tf.nn.relu)(h1)\n",
    "    logits = tf.layers.Dense(29)(h2)\n",
    "    \n",
    "    \n",
    "    predictions = {\n",
    "        \"class_ids\": tf.argmax(input=logits, axis=1)\n",
    "    }\n",
    "\n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        return tf.estimator.EstimatorSpec(mode=mode, predictions=predictions)\n",
    "\n",
    "    loss = tf.losses.sparse_softmax_cross_entropy(labels=labels,\n",
    "                                                  logits = logits)\n",
    "    \n",
    "    accuracy = tf.metrics.accuracy(labels, predictions['class_ids'])\n",
    "\n",
    "    if mode == tf.estimator.ModeKeys.EVAL:\n",
    "        return tf.estimator.EstimatorSpec(\n",
    "            mode,\n",
    "            loss=loss,\n",
    "            eval_metric_ops={'my_accuracy': accuracy})\n",
    "\n",
    "    # If mode is not PREDICT nor EVAL, then we must be in TRAIN\n",
    "    assert mode == tf.estimator.ModeKeys.TRAIN, \"TRAIN is only ModeKey left\"    \n",
    "    \n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.001)\n",
    "    train_op = optimizer.minimize(\n",
    "        loss=loss,\n",
    "        global_step=tf.train.get_global_step())\n",
    "    \n",
    "    tf.summary.scalar('my_accuracy', accuracy[1])\n",
    "\n",
    "    \n",
    "    return tf.estimator.EstimatorSpec(mode=mode, loss=loss, train_op=train_op)\n",
    "   \n",
    "    # Add evaluation metrics (for EVAL mode)\n",
    "#    eval_metric_ops = {\n",
    "#            \"accuracy\": tf.metrics.accuracy(\n",
    "#                    labels=labels, predictions=predictions[\"classes\"])}\n",
    "    \n",
    "#    return tf.estimator.EstimatorSpec(\n",
    "#            mode=mode, \n",
    "#            loss=loss, \n",
    "#            eval_metric_ops=eval_metric_ops)\n",
    "\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(replicons_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using config: {'_task_type': None, '_task_id': 0, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x000001E45F8C66D8>, '_master': '', '_num_ps_replicas': 0, '_num_worker_replicas': 0, '_environment': 'local', '_is_chief': True, '_evaluation_master': '', '_tf_config': gpu_options {\n",
      "  per_process_gpu_memory_fraction: 1\n",
      "}\n",
      ", '_tf_random_seed': None, '_save_summary_steps': 5, '_save_checkpoints_secs': None, '_log_step_count_steps': 100, '_session_config': None, '_save_checkpoints_steps': 10, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_model_dir': 'classifier_glove_cnn4.2'}\n",
      "INFO:tensorflow:my_model_fn: TRAIN, train\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from classifier_glove_cnn4.2\\model.ckpt-300\n",
      "Loading from pickle: training-files//WSM419.final.fasta\n",
      "INFO:tensorflow:Saving checkpoints for 301 into classifier_glove_cnn4.2\\model.ckpt.\n",
      "INFO:tensorflow:loss = 3.2793221, step = 301\n",
      "INFO:tensorflow:Saving checkpoints for 311 into classifier_glove_cnn4.2\\model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 321 into classifier_glove_cnn4.2\\model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 331 into classifier_glove_cnn4.2\\model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 341 into classifier_glove_cnn4.2\\model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 351 into classifier_glove_cnn4.2\\model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 361 into classifier_glove_cnn4.2\\model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 371 into classifier_glove_cnn4.2\\model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 381 into classifier_glove_cnn4.2\\model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 391 into classifier_glove_cnn4.2\\model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 401 into classifier_glove_cnn4.2\\model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 3.1264\n",
      "INFO:tensorflow:loss = 3.1837664, step = 401 (31.989 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 411 into classifier_glove_cnn4.2\\model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 421 into classifier_glove_cnn4.2\\model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 431 into classifier_glove_cnn4.2\\model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 441 into classifier_glove_cnn4.2\\model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 451 into classifier_glove_cnn4.2\\model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 461 into classifier_glove_cnn4.2\\model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 471 into classifier_glove_cnn4.2\\model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 481 into classifier_glove_cnn4.2\\model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 491 into classifier_glove_cnn4.2\\model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 501 into classifier_glove_cnn4.2\\model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 3.15402\n",
      "INFO:tensorflow:loss = 3.088994, step = 501 (31.703 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 511 into classifier_glove_cnn4.2\\model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 521 into classifier_glove_cnn4.2\\model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 531 into classifier_glove_cnn4.2\\model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 541 into classifier_glove_cnn4.2\\model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 551 into classifier_glove_cnn4.2\\model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 561 into classifier_glove_cnn4.2\\model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 571 into classifier_glove_cnn4.2\\model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 581 into classifier_glove_cnn4.2\\model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 591 into classifier_glove_cnn4.2\\model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 601 into classifier_glove_cnn4.2\\model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 3.14901\n",
      "INFO:tensorflow:loss = 2.9950721, step = 601 (31.756 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 611 into classifier_glove_cnn4.2\\model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 621 into classifier_glove_cnn4.2\\model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 631 into classifier_glove_cnn4.2\\model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 641 into classifier_glove_cnn4.2\\model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 651 into classifier_glove_cnn4.2\\model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 661 into classifier_glove_cnn4.2\\model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 671 into classifier_glove_cnn4.2\\model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 681 into classifier_glove_cnn4.2\\model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 691 into classifier_glove_cnn4.2\\model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 701 into classifier_glove_cnn4.2\\model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 3.0425\n",
      "INFO:tensorflow:loss = 2.9020698, step = 701 (32.868 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 711 into classifier_glove_cnn4.2\\model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 721 into classifier_glove_cnn4.2\\model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 731 into classifier_glove_cnn4.2\\model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 741 into classifier_glove_cnn4.2\\model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 751 into classifier_glove_cnn4.2\\model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 761 into classifier_glove_cnn4.2\\model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 771 into classifier_glove_cnn4.2\\model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 781 into classifier_glove_cnn4.2\\model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 791 into classifier_glove_cnn4.2\\model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 801 into classifier_glove_cnn4.2\\model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 3.05828\n",
      "INFO:tensorflow:loss = 2.810063, step = 801 (32.698 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 811 into classifier_glove_cnn4.2\\model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 821 into classifier_glove_cnn4.2\\model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 831 into classifier_glove_cnn4.2\\model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 841 into classifier_glove_cnn4.2\\model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 851 into classifier_glove_cnn4.2\\model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 861 into classifier_glove_cnn4.2\\model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 871 into classifier_glove_cnn4.2\\model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 881 into classifier_glove_cnn4.2\\model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 891 into classifier_glove_cnn4.2\\model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 901 into classifier_glove_cnn4.2\\model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 3.0978\n",
      "INFO:tensorflow:loss = 2.7191284, step = 901 (32.281 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 911 into classifier_glove_cnn4.2\\model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 921 into classifier_glove_cnn4.2\\model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 931 into classifier_glove_cnn4.2\\model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 941 into classifier_glove_cnn4.2\\model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 951 into classifier_glove_cnn4.2\\model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 961 into classifier_glove_cnn4.2\\model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 971 into classifier_glove_cnn4.2\\model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 981 into classifier_glove_cnn4.2\\model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 991 into classifier_glove_cnn4.2\\model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 1001 into classifier_glove_cnn4.2\\model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 3.02382\n",
      "INFO:tensorflow:loss = 2.6293461, step = 1001 (33.071 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1011 into classifier_glove_cnn4.2\\model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 1021 into classifier_glove_cnn4.2\\model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 1031 into classifier_glove_cnn4.2\\model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 1041 into classifier_glove_cnn4.2\\model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 1051 into classifier_glove_cnn4.2\\model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 1061 into classifier_glove_cnn4.2\\model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 1071 into classifier_glove_cnn4.2\\model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 1081 into classifier_glove_cnn4.2\\model.ckpt.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saving checkpoints for 1091 into classifier_glove_cnn4.2\\model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 1101 into classifier_glove_cnn4.2\\model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 3.10521\n",
      "INFO:tensorflow:loss = 2.5408006, step = 1101 (32.204 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1111 into classifier_glove_cnn4.2\\model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 1121 into classifier_glove_cnn4.2\\model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 1131 into classifier_glove_cnn4.2\\model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 1141 into classifier_glove_cnn4.2\\model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 1151 into classifier_glove_cnn4.2\\model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 1161 into classifier_glove_cnn4.2\\model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 1171 into classifier_glove_cnn4.2\\model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 1181 into classifier_glove_cnn4.2\\model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 1191 into classifier_glove_cnn4.2\\model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 1201 into classifier_glove_cnn4.2\\model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 3.06097\n",
      "INFO:tensorflow:loss = 2.453577, step = 1201 (32.669 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1211 into classifier_glove_cnn4.2\\model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 1221 into classifier_glove_cnn4.2\\model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 1231 into classifier_glove_cnn4.2\\model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 1241 into classifier_glove_cnn4.2\\model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 1251 into classifier_glove_cnn4.2\\model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 1261 into classifier_glove_cnn4.2\\model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 1271 into classifier_glove_cnn4.2\\model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 1281 into classifier_glove_cnn4.2\\model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 1291 into classifier_glove_cnn4.2\\model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 1301 into classifier_glove_cnn4.2\\model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 3.02978\n",
      "INFO:tensorflow:loss = 2.3677616, step = 1301 (33.006 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1311 into classifier_glove_cnn4.2\\model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 1321 into classifier_glove_cnn4.2\\model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 1331 into classifier_glove_cnn4.2\\model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 1341 into classifier_glove_cnn4.2\\model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 1351 into classifier_glove_cnn4.2\\model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 1361 into classifier_glove_cnn4.2\\model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 1371 into classifier_glove_cnn4.2\\model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 1381 into classifier_glove_cnn4.2\\model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 1391 into classifier_glove_cnn4.2\\model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 1401 into classifier_glove_cnn4.2\\model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 2.98364\n",
      "INFO:tensorflow:loss = 2.283443, step = 1401 (33.516 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1411 into classifier_glove_cnn4.2\\model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 1421 into classifier_glove_cnn4.2\\model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 1431 into classifier_glove_cnn4.2\\model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 1441 into classifier_glove_cnn4.2\\model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 1451 into classifier_glove_cnn4.2\\model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 1461 into classifier_glove_cnn4.2\\model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 1471 into classifier_glove_cnn4.2\\model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 1481 into classifier_glove_cnn4.2\\model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 1491 into classifier_glove_cnn4.2\\model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 1501 into classifier_glove_cnn4.2\\model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 2.99374\n",
      "INFO:tensorflow:loss = 2.2007077, step = 1501 (33.403 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1511 into classifier_glove_cnn4.2\\model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 1521 into classifier_glove_cnn4.2\\model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 1531 into classifier_glove_cnn4.2\\model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 1541 into classifier_glove_cnn4.2\\model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 1551 into classifier_glove_cnn4.2\\model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 1561 into classifier_glove_cnn4.2\\model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 1571 into classifier_glove_cnn4.2\\model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 1581 into classifier_glove_cnn4.2\\model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 1591 into classifier_glove_cnn4.2\\model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 1601 into classifier_glove_cnn4.2\\model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 3.10296\n",
      "INFO:tensorflow:loss = 2.1196437, step = 1601 (32.227 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1611 into classifier_glove_cnn4.2\\model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 1621 into classifier_glove_cnn4.2\\model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 1631 into classifier_glove_cnn4.2\\model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 1641 into classifier_glove_cnn4.2\\model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 1651 into classifier_glove_cnn4.2\\model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 1661 into classifier_glove_cnn4.2\\model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 1671 into classifier_glove_cnn4.2\\model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 1681 into classifier_glove_cnn4.2\\model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 1691 into classifier_glove_cnn4.2\\model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 1701 into classifier_glove_cnn4.2\\model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 2.93645\n",
      "INFO:tensorflow:loss = 2.0403342, step = 1701 (34.055 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1711 into classifier_glove_cnn4.2\\model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 1721 into classifier_glove_cnn4.2\\model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 1731 into classifier_glove_cnn4.2\\model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 1741 into classifier_glove_cnn4.2\\model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 1751 into classifier_glove_cnn4.2\\model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 1761 into classifier_glove_cnn4.2\\model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 1771 into classifier_glove_cnn4.2\\model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 1781 into classifier_glove_cnn4.2\\model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 1791 into classifier_glove_cnn4.2\\model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 1801 into classifier_glove_cnn4.2\\model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 2.87626\n",
      "INFO:tensorflow:loss = 1.9628611, step = 1801 (34.767 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1811 into classifier_glove_cnn4.2\\model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 1821 into classifier_glove_cnn4.2\\model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 1831 into classifier_glove_cnn4.2\\model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 1841 into classifier_glove_cnn4.2\\model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 1851 into classifier_glove_cnn4.2\\model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 1861 into classifier_glove_cnn4.2\\model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 1871 into classifier_glove_cnn4.2\\model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 1881 into classifier_glove_cnn4.2\\model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 1891 into classifier_glove_cnn4.2\\model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 1901 into classifier_glove_cnn4.2\\model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 2.87842\n",
      "INFO:tensorflow:loss = 1.8873008, step = 1901 (34.741 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1911 into classifier_glove_cnn4.2\\model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 1921 into classifier_glove_cnn4.2\\model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 1931 into classifier_glove_cnn4.2\\model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 1941 into classifier_glove_cnn4.2\\model.ckpt.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saving checkpoints for 1951 into classifier_glove_cnn4.2\\model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 1961 into classifier_glove_cnn4.2\\model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 1971 into classifier_glove_cnn4.2\\model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 1981 into classifier_glove_cnn4.2\\model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 1991 into classifier_glove_cnn4.2\\model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 2001 into classifier_glove_cnn4.2\\model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 3.03024\n",
      "INFO:tensorflow:loss = 1.8137262, step = 2001 (33.001 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 2011 into classifier_glove_cnn4.2\\model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 2021 into classifier_glove_cnn4.2\\model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 2031 into classifier_glove_cnn4.2\\model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 2041 into classifier_glove_cnn4.2\\model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 2051 into classifier_glove_cnn4.2\\model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 2061 into classifier_glove_cnn4.2\\model.ckpt.\n"
     ]
    }
   ],
   "source": [
    "classifier = tf.estimator.Estimator(\n",
    "    model_fn=cnn_model_fn,\n",
    "    model_dir=\"classifier_glove_cnn4.2\",\n",
    "    config=tf.contrib.learn.RunConfig(\n",
    "        save_checkpoints_steps=10,\n",
    "        save_checkpoints_secs=None,\n",
    "        save_summary_steps=5))\n",
    "\n",
    "classifier.train(input_fn=input_fn, steps=10000)\n",
    "classifier.evaluate(input_fn=input_fn, steps=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ignore below for now..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(unused_argv):\n",
    "    classifier = tf.estimator.Estimator(\n",
    "            model_fn=cnn_model_fn,\n",
    "            model_dir=\"classifier_glove_cnn4\",\n",
    "            config=tf.contrib.learn.RunConfig(\n",
    "                    save_checkpoints_steps=10,\n",
    "                    save_checkpoints_secs=None,\n",
    "                    save_summary_steps=5))\n",
    "    \n",
    "    classifier.train(input_fn=input_fn, steps=10)\n",
    "    \n",
    "    # eval_results = classifier.evaluate(input_fn=my_input_fn, steps=10)\n",
    "    # print(eval_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using config: {'_task_type': None, '_task_id': 0, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x0000022ECEACC9E8>, '_master': '', '_num_ps_replicas': 0, '_num_worker_replicas': 0, '_environment': 'local', '_is_chief': True, '_evaluation_master': '', '_tf_config': gpu_options {\n",
      "  per_process_gpu_memory_fraction: 1\n",
      "}\n",
      ", '_tf_random_seed': None, '_save_summary_steps': 5, '_save_checkpoints_secs': None, '_log_step_count_steps': 100, '_session_config': None, '_save_checkpoints_steps': 10, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_model_dir': 'classifier_glove_cnn4'}\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "  tf.app.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using pre-made models starts down here...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmers_fc = tf.feature_column.numeric_column(key=\"kmers\", shape=15, dtype=tf.int64)\n",
    "\n",
    "kmers_dict = tf.feature_column.categorical_column_with_vocabulary_list(\n",
    "    key=\"kmers\",\n",
    "    vocabulary_list=list(map(int, vocab.keys())))\n",
    "\n",
    "kmers_dict_1 = tf.feature_column.categorical_column_with_vocabulary_list(\n",
    "    key=\"kmer_1\",\n",
    "    vocabulary_list=list(map(int, vocab.keys())))\n",
    "\n",
    "kmers_dict_2 = tf.feature_column.categorical_column_with_vocabulary_list(\n",
    "    key=\"kmer_2\",\n",
    "    vocabulary_list=list(map(int, vocab.keys())))\n",
    "\n",
    "kmers_dict_3 = tf.feature_column.categorical_column_with_vocabulary_list(\n",
    "    key=\"kmer_3\",\n",
    "    vocabulary_list=list(map(int, vocab.keys())))\n",
    "\n",
    "kmers_dict_4 = tf.feature_column.categorical_column_with_vocabulary_list(\n",
    "    key=\"kmer_4\",\n",
    "    vocabulary_list=list(map(int, vocab.keys())))\n",
    "\n",
    "kmers_dict_5 = tf.feature_column.categorical_column_with_vocabulary_list(\n",
    "    key=\"kmer_5\",\n",
    "    vocabulary_list=list(map(int, vocab.keys())))\n",
    "\n",
    "kmers_dict_6 = tf.feature_column.categorical_column_with_vocabulary_list(\n",
    "    key=\"kmer_6\",\n",
    "    vocabulary_list=list(map(int, vocab.keys())))\n",
    "\n",
    "kmers_dict_7 = tf.feature_column.categorical_column_with_vocabulary_list(\n",
    "    key=\"kmer_7\",\n",
    "    vocabulary_list=list(map(int, vocab.keys())))\n",
    "\n",
    "kmers_dict_8 = tf.feature_column.categorical_column_with_vocabulary_list(\n",
    "    key=\"kmer_8\",\n",
    "    vocabulary_list=list(map(int, vocab.keys())))\n",
    "\n",
    "kmers_dict_9 = tf.feature_column.categorical_column_with_vocabulary_list(\n",
    "    key=\"kmer_9\",\n",
    "    vocabulary_list=list(map(int, vocab.keys())))\n",
    "\n",
    "kmers_dict_10 = tf.feature_column.categorical_column_with_vocabulary_list(\n",
    "    key=\"kmer_10\",\n",
    "    vocabulary_list=list(map(int, vocab.keys())))\n",
    "\n",
    "kmers_dict_11 = tf.feature_column.categorical_column_with_vocabulary_list(\n",
    "    key=\"kmer_11\",\n",
    "    vocabulary_list=list(map(int, vocab.keys())))\n",
    "\n",
    "kmers_dict_12 = tf.feature_column.categorical_column_with_vocabulary_list(\n",
    "    key=\"kmer_12\",\n",
    "    vocabulary_list=list(map(int, vocab.keys())))\n",
    "\n",
    "kmers_dict_13 = tf.feature_column.categorical_column_with_vocabulary_list(\n",
    "    key=\"kmer_13\",\n",
    "    vocabulary_list=list(map(int, vocab.keys())))\n",
    "\n",
    "kmers_dict_14 = tf.feature_column.categorical_column_with_vocabulary_list(\n",
    "    key=\"kmer_14\",\n",
    "    vocabulary_list=list(map(int, vocab.keys())))\n",
    "\n",
    "kmers_dict_15 = tf.feature_column.categorical_column_with_vocabulary_list(\n",
    "    key=\"kmer_15\",\n",
    "    vocabulary_list=list(map(int, vocab.keys())))\n",
    "\n",
    "kmers_fc_embed_1 = tf.feature_column.embedding_column(\n",
    "    categorical_column=kmers_dict_1, \n",
    "    dimension=256,\n",
    "    initializer=init,\n",
    "    trainable=False)\n",
    "\n",
    "kmers_fc_embed_2 = tf.feature_column.embedding_column(\n",
    "    categorical_column=kmers_dict_2, \n",
    "    dimension=256,\n",
    "    initializer=init,\n",
    "    trainable=False)\n",
    "\n",
    "kmers_fc_embed_3 = tf.feature_column.embedding_column(\n",
    "    categorical_column=kmers_dict_3, \n",
    "    dimension=256,\n",
    "    initializer=init,\n",
    "    trainable=False)\n",
    "\n",
    "kmers_fc_embed_4 = tf.feature_column.embedding_column(\n",
    "    categorical_column=kmers_dict_4, \n",
    "    dimension=256,\n",
    "    initializer=init,\n",
    "    trainable=False)\n",
    "\n",
    "kmers_fc_embed_5 = tf.feature_column.embedding_column(\n",
    "    categorical_column=kmers_dict_5, \n",
    "    dimension=256,\n",
    "    initializer=init,\n",
    "    trainable=False)\n",
    "\n",
    "kmers_fc_embed_6 = tf.feature_column.embedding_column(\n",
    "    categorical_column=kmers_dict_6, \n",
    "    dimension=256,\n",
    "    initializer=init,\n",
    "    trainable=False)\n",
    "\n",
    "kmers_fc_embed_7 = tf.feature_column.embedding_column(\n",
    "    categorical_column=kmers_dict_7, \n",
    "    dimension=256,\n",
    "    initializer=init,\n",
    "    trainable=False)\n",
    "\n",
    "kmers_fc_embed_8 = tf.feature_column.embedding_column(\n",
    "    categorical_column=kmers_dict_8, \n",
    "    dimension=256,\n",
    "    initializer=init,\n",
    "    trainable=False)\n",
    "\n",
    "kmers_fc_embed_9 = tf.feature_column.embedding_column(\n",
    "    categorical_column=kmers_dict_9, \n",
    "    dimension=256,\n",
    "    initializer=init,\n",
    "    trainable=False)\n",
    "\n",
    "kmers_fc_embed_10 = tf.feature_column.embedding_column(\n",
    "    categorical_column=kmers_dict_10, \n",
    "    dimension=256,\n",
    "    initializer=init,\n",
    "    trainable=False)\n",
    "\n",
    "kmers_fc_embed_11 = tf.feature_column.embedding_column(\n",
    "    categorical_column=kmers_dict_11, \n",
    "    dimension=256,\n",
    "    initializer=init,\n",
    "    trainable=False)\n",
    "\n",
    "kmers_fc_embed_12 = tf.feature_column.embedding_column(\n",
    "    categorical_column=kmers_dict_12, \n",
    "    dimension=256,\n",
    "    initializer=init,\n",
    "    trainable=False)\n",
    "\n",
    "kmers_fc_embed_13 = tf.feature_column.embedding_column(\n",
    "    categorical_column=kmers_dict_13, \n",
    "    dimension=256,\n",
    "    initializer=init,\n",
    "    trainable=False)\n",
    "\n",
    "kmers_fc_embed_14 = tf.feature_column.embedding_column(\n",
    "    categorical_column=kmers_dict_14, \n",
    "    dimension=256,\n",
    "    initializer=init,\n",
    "    trainable=False)\n",
    "\n",
    "kmers_fc_embed_15 = tf.feature_column.embedding_column(\n",
    "    categorical_column=kmers_dict_15, \n",
    "    dimension=256,\n",
    "    initializer=init,\n",
    "    trainable=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#kmers_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [kmers_fc_embed_1, kmers_fc_embed_2, kmers_fc_embed_3, kmers_fc_embed_4, kmers_fc_embed_5,\n",
    "           kmers_fc_embed_6, kmers_fc_embed_7, kmers_fc_embed_8, kmers_fc_embed_9, kmers_fc_embed_10,\n",
    "           kmers_fc_embed_11, kmers_fc_embed_12, kmers_fc_embed_13, kmers_fc_embed_14, kmers_fc_embed_15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "WARNING:tensorflow:Using temporary folder as model directory: C:\\Users\\Joey\\AppData\\Local\\Temp\\tmp8nt_njqc\n",
      "INFO:tensorflow:Using config: {'_model_dir': 'C:\\\\Users\\\\Joey\\\\AppData\\\\Local\\\\Temp\\\\tmp8nt_njqc', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x000001E49D4C9160>, '_task_type': 'worker', '_task_id': 0, '_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
     ]
    }
   ],
   "source": [
    "estimator = tf.estimator.DNNClassifier(feature_columns=columns,\n",
    "                                      hidden_units=[1024,512,256])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Create CheckpointSaverHook.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "GraphDef cannot be larger than 2GB.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-32-72ad0ac385d3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_fn_new\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msteps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\estimator\\estimator.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, input_fn, hooks, steps, max_steps, saving_listeners)\u001b[0m\n\u001b[0;32m    300\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    301\u001b[0m     \u001b[0msaving_listeners\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_check_listeners_type\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msaving_listeners\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 302\u001b[1;33m     \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_train_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    303\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Loss for final step: %s.'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    304\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\estimator\\estimator.py\u001b[0m in \u001b[0;36m_train_model\u001b[1;34m(self, input_fn, hooks, saving_listeners)\u001b[0m\n\u001b[0;32m    778\u001b[0m           \u001b[0msave_summaries_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_config\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave_summary_steps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    779\u001b[0m           \u001b[0mconfig\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session_config\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 780\u001b[1;33m           log_step_count_steps=self._config.log_step_count_steps) as mon_sess:\n\u001b[0m\u001b[0;32m    781\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    782\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mmon_sess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_stop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\monitored_session.py\u001b[0m in \u001b[0;36mMonitoredTrainingSession\u001b[1;34m(master, is_chief, checkpoint_dir, scaffold, hooks, chief_only_hooks, save_checkpoint_secs, save_summaries_steps, save_summaries_secs, config, stop_grace_period_secs, log_step_count_steps)\u001b[0m\n\u001b[0;32m    366\u001b[0m     \u001b[0mall_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhooks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    367\u001b[0m   return MonitoredSession(session_creator=session_creator, hooks=all_hooks,\n\u001b[1;32m--> 368\u001b[1;33m                           stop_grace_period_secs=stop_grace_period_secs)\n\u001b[0m\u001b[0;32m    369\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    370\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\monitored_session.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, session_creator, hooks, stop_grace_period_secs)\u001b[0m\n\u001b[0;32m    671\u001b[0m     super(MonitoredSession, self).__init__(\n\u001b[0;32m    672\u001b[0m         \u001b[0msession_creator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshould_recover\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 673\u001b[1;33m         stop_grace_period_secs=stop_grace_period_secs)\n\u001b[0m\u001b[0;32m    674\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    675\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\monitored_session.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, session_creator, hooks, should_recover, stop_grace_period_secs)\u001b[0m\n\u001b[0;32m    484\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhooks\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    485\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mh\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_hooks\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 486\u001b[1;33m       \u001b[0mh\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbegin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    487\u001b[0m     \u001b[1;31m# Create the session.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    488\u001b[0m     self._coordinated_creator = self._CoordinatedSessionCreator(\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\basic_session_run_hooks.py\u001b[0m in \u001b[0;36mbegin\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    416\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    417\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mbegin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 418\u001b[1;33m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_summary_writer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSummaryWriterCache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_checkpoint_dir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    419\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_global_step_tensor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtraining_util\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_or_create_global_step_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    420\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_global_step_tensor\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\summary\\writer\\writer_cache.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(logdir)\u001b[0m\n\u001b[0;32m     59\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mlogdir\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mFileWriterCache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_cache\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m         FileWriterCache._cache[logdir] = FileWriter(\n\u001b[1;32m---> 61\u001b[1;33m             logdir, graph=ops.get_default_graph())\n\u001b[0m\u001b[0;32m     62\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mFileWriterCache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_cache\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlogdir\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\summary\\writer\\writer.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, logdir, graph, max_queue, flush_secs, graph_def, filename_suffix)\u001b[0m\n\u001b[0;32m    335\u001b[0m     event_writer = EventFileWriter(logdir, max_queue, flush_secs,\n\u001b[0;32m    336\u001b[0m                                    filename_suffix)\n\u001b[1;32m--> 337\u001b[1;33m     \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mFileWriter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mevent_writer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgraph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgraph_def\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    338\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    339\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\summary\\writer\\writer.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, event_writer, graph, graph_def)\u001b[0m\n\u001b[0;32m     78\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mgraph\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mgraph_def\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     79\u001b[0m       \u001b[1;31m# Calling it with both graph and graph_def for backward compatibility.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 80\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgraph_def\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgraph_def\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     81\u001b[0m       \u001b[1;31m# Also export the meta_graph_def in this case.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m       \u001b[1;31m# graph may itself be a graph_def due to positional arguments\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\summary\\writer\\writer.py\u001b[0m in \u001b[0;36madd_graph\u001b[1;34m(self, graph, global_step, graph_def)\u001b[0m\n\u001b[0;32m    188\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    189\u001b[0m       \u001b[1;31m# Serialize the graph with additional info.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 190\u001b[1;33m       \u001b[0mtrue_graph_def\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgraph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_graph_def\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0madd_shapes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    191\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_write_plugin_assets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    192\u001b[0m     elif (isinstance(graph, graph_pb2.GraphDef) or\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mas_graph_def\u001b[1;34m(self, from_version, add_shapes)\u001b[0m\n\u001b[0;32m   2771\u001b[0m     \"\"\"\n\u001b[0;32m   2772\u001b[0m     \u001b[1;31m# pylint: enable=line-too-long\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2773\u001b[1;33m     \u001b[0mresult\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_as_graph_def\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfrom_version\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0madd_shapes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2774\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2775\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m_as_graph_def\u001b[1;34m(self, from_version, add_shapes)\u001b[0m\n\u001b[0;32m   2731\u001b[0m           \u001b[0mbytesize\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mByteSize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2732\u001b[0m           \u001b[1;32mif\u001b[0m \u001b[0mbytesize\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m<<\u001b[0m \u001b[1;36m31\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mbytesize\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2733\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"GraphDef cannot be larger than 2GB.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2734\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_functions\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2735\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_functions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: GraphDef cannot be larger than 2GB."
     ]
    }
   ],
   "source": [
    "estimator.train(input_fn=input_fn_new, steps=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'kmers': <tf.Tensor 'IteratorGetNext_7:0' shape=(?, 15) dtype=float32>}"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_fn_new():\n",
    "    kmer_gen = functools.partial(kmer_generator, \"training-files/\", 7)\n",
    "\n",
    "    ds = tf.data.Dataset.from_generator(kmer_gen, \n",
    "                                        (tf.int64,\n",
    "                                         tf.int64),\n",
    "                                        (tf.TensorShape(15),\n",
    "                                         tf.TensorShape(None)))\n",
    "                                        \n",
    "#    # Numbers reduced to run on my desktop\n",
    "#    ds = ds.repeat(5)\n",
    "#    ds = ds.prefetch(5000) # Each batch is only 2048, so prefetch 5000\n",
    "#    ds = ds.shuffle(buffer_size=1000000) # Large buffer size for better randomization\n",
    "#    ds = ds.batch(2048) # Reduced from 5000 so it runs quicker\n",
    "    \n",
    "#    ds = ds.repeat(1)\n",
    "#    ds = ds.prefetch(2)\n",
    "#    ds = ds.shuffle(buffer_size=500)\n",
    "    ds = ds.batch(1)\n",
    "    \n",
    "    def add_labels(arr, lab):\n",
    "        return({\"kmer_1\": arr[0],\n",
    "                \"kmer_2\": arr[1],\n",
    "                \"kmer_3\": arr[2],\n",
    "                \"kmer_4\": arr[3],\n",
    "                \"kmer_5\": arr[4],\n",
    "                \"kmer_6\": arr[5],\n",
    "                \"kmer_7\": arr[6],\n",
    "                \"kmer_8\": arr[7],\n",
    "                \"kmer_9\": arr[8],\n",
    "                \"kmer_10\": arr[9],\n",
    "                \"kmer_11\": arr[10],\n",
    "                \"kmer_12\": arr[11],\n",
    "                \"kmer_13\": arr[12],\n",
    "                \"kmer_14\": arr[13],\n",
    "                \"kmer_15\": arr[14]}, lab)\n",
    "    \n",
    "    ds = ds.map(add_labels)\n",
    "    iterator = ds.make_one_shot_iterator()\n",
    "    batch_features, batch_labels = iterator.get_next()\n",
    "    return batch_features, batch_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def add_labels(arr, lab):\n",
    "        return({\"kmer_1\": arr[0],\n",
    "                \"kmer_2\": arr[1],\n",
    "                \"kmer_3\": arr[2],\n",
    "                \"kmer_4\": arr[3],\n",
    "                \"kmer_5\": arr[4],\n",
    "                \"kmer_6\": arr[5],\n",
    "                \"kmer_7\": arr[6],\n",
    "                \"kmer_8\": arr[7],\n",
    "                \"kmer_9\": arr[8],\n",
    "                \"kmer_10\": arr[9],\n",
    "                \"kmer_11\": arr[10],\n",
    "                \"kmer_12\": arr[11],\n",
    "                \"kmer_13\": arr[12],\n",
    "                \"kmer_14\": arr[13],\n",
    "                \"kmer_15\": arr[14]}, lab)\n",
    "    \n",
    "    ds = tf.data.Dataset.from_generator(kmer_gen, \n",
    "                                        (tf.int64,\n",
    "                                         tf.int64),\n",
    "                                        (tf.TensorShape([15]),\n",
    "                                         tf.TensorShape(None)))\n",
    "    \n",
    "    ds = ds.map(add_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterator = ds.make_one_shot_iterator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_features, batch_labels = iterator.get_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init(shape=None,\n",
    "         dtype=None,\n",
    "         partition_info=None):\n",
    "    return W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([15], dtype=int64)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Previous classifier\n",
    "\n",
    "def cnn_model_fn(features, labels, mode):\n",
    "    \"\"\"Model for CNN\"\"\"\n",
    "    \n",
    "    Weights = tf.Variable(tf.constant(0.0, shape=[vocab_size, embedding_dim]), trainable=False, name=\"Weights\")\n",
    "    \n",
    "    embedding_placeholder = tf.placeholder(tf.float32, [vocab_size, embedding_dim])\n",
    "    embedding_init = Weights.assign(embedding_placeholder)\n",
    "    \n",
    "    def init_fn(scaffold, sess):\n",
    "        sess.run(Weights.initializer, {Weights.initial_value: W})\n",
    "    scaffold = tf.train.Scaffold(init_fn=init_fn)\n",
    "\n",
    "    inputs = tf.feature_column.input_layer(features, [kmers_fc])\n",
    "    \n",
    "    input_i64 = tf.cast(inputs, tf.int64)\n",
    "    \n",
    "    embedded_kmers = tf.nn.embedding_lookup(Weights, input_i64)\n",
    "    \n",
    "    input_layer = tf.reshape(embedded_kmers, [-1, 15, 256, 1])\n",
    "    \n",
    "    input_layer = tf.cast(input_layer, tf.float32)\n",
    "    \n",
    "    conv1 = tf.layers.conv2d(inputs = input_layer,\n",
    "                             filters=32,\n",
    "                             kernel_size=[2,256],\n",
    "                             strides=3,\n",
    "                             padding=\"same\",\n",
    "                             name=\"Conv1\",\n",
    "                             activation=None)\n",
    "    \n",
    "    avg_pool1 = tf.layers.average_pooling2d(conv1, \n",
    "                                            pool_size=[4,32], \n",
    "                                            strides=[2,16],\n",
    "                                            padding=\"same\",\n",
    "                                            name=\"AvgPooling_1\")\n",
    "    \n",
    "    logits = tf.layers.dense(inputs=avg_pool1, units=len(replicons_list))\n",
    "    \n",
    "    predictions = {\n",
    "        \"classes\": tf.argmax(input=logits, axis=1),\n",
    "        \"probabilities\": tf.nn.softmax(logits, name=\"softmax_tensor\")}\n",
    "    \n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        return tf.estimator.EstimatorSpec(mode=mode, predictions=predictions)\n",
    "\n",
    "    onehot_labels = tf.one_hot(indices=tf.cast(labels, tf.int32),\n",
    "                              depth=len(replicons_list))\n",
    "    \n",
    "    correct_prediction = tf.equal(tf.argmax(logits, 1), labels)\n",
    "    \n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float64))\n",
    "    \n",
    "    tf.summary.scalar(\"Accuracy\", accuracy)\n",
    "    \n",
    "    # labels = tf.squeeze(labels, 1)\n",
    "    loss = tf.losses.sparse_softmax_cross_entropy(labels=labels,\n",
    "                                                  logits = logits\n",
    "                                                 )\n",
    "    \n",
    "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "        optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.001)\n",
    "        train_op = optimizer.minimize(\n",
    "                loss=loss,\n",
    "                global_step=tf.train.get_global_step())\n",
    "        return tf.estimator.EstimatorSpec(mode=mode, loss=loss, train_op=train_op)\n",
    "   \n",
    "    # Add evaluation metrics (for EVAL mode)\n",
    "    eval_metric_ops = {\n",
    "            \"accuracy\": tf.metrics.accuracy(\n",
    "                    labels=labels, predictions=predictions[\"classes\"])}\n",
    "    \n",
    "    return tf.estimator.EstimatorSpec(\n",
    "            mode=mode, \n",
    "            loss=loss, \n",
    "            eval_metric_ops=eval_metric_ops)\n",
    "\n",
    "\n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
