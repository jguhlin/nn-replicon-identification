{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[269132, 1683993, 1953125]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.sparse import csr_matrix\n",
    "from collections import Counter\n",
    "import collections\n",
    "import random\n",
    "from six.moves import urllib\n",
    "from six.moves import xrange  # pylint: disable=redefined-builtin\n",
    "import Bio\n",
    "from Bio import SeqIO\n",
    "import os\n",
    "import concurrent.futures\n",
    "import functools\n",
    "from functools import partial\n",
    "import math\n",
    "import threading\n",
    "import time\n",
    "import random\n",
    "from random import shuffle\n",
    "import pickle\n",
    "import tempfile\n",
    "import ntpath\n",
    "import os.path\n",
    "\n",
    "# k-mer size to use\n",
    "k = 9\n",
    "\n",
    "#\n",
    "# NOTE!!!!!!!!!!!!!!!!\n",
    "#\n",
    "# We can reduce problem space if we get the reverse complement, and add a bit to indicate reversed or not...\n",
    "# Not really.... revcomp just doubles it back up again....\n",
    "#\n",
    "# Also -- Build a recurrent network to predict sequences that come after a given kmer?\n",
    "# Look at word2vec, dna2vec, bag of words, skip-gram\n",
    "#\n",
    "\n",
    "# Problem space\n",
    "space = 5 ** k\n",
    "\n",
    "def partition(n, step, coll):\n",
    "    for i in range(0, len(coll), step):\n",
    "        if (i+n > len(coll)):\n",
    "            break #  raise StopIteration...\n",
    "        yield coll[i:i+n]\n",
    "        \n",
    "def get_kmers(k):\n",
    "    return lambda sequence: partition(k, k, sequence)\n",
    "\n",
    "def convert_nt(c):\n",
    "    return {\"N\": 0, \"A\": 1, \"C\": 2, \"T\": 3, \"G\": 4}.get(c, 0)\n",
    "\n",
    "def convert_nt_complement(c):\n",
    "    return {\"N\": 0, \"A\": 3, \"C\": 4, \"T\": 1, \"G\": 2}.get(c, 0)\n",
    "\n",
    "def convert_kmer_to_int(kmer):\n",
    "    return int(''.join(str(x) for x in (map(convert_nt, kmer))), 5)\n",
    "\n",
    "def convert_kmer_to_int_complement(kmer):\n",
    "    return int(''.join(str(x) for x in reversed(list(map(convert_nt_complement, kmer)))), 5)\n",
    "\n",
    "def convert_base5(n):\n",
    "    return {\"0\": \"N\", \"1\": \"A\", \"2\": \"C\", \"3\": \"T\", \"4\": \"G\"}.get(n,\"N\")\n",
    "\n",
    "def convert_to_kmer(kmer):\n",
    "    return ''.join(map(convert_base5, str(np.base_repr(kmer, 5))))\n",
    "\n",
    "# Not using sparse tensors anymore.\n",
    "   \n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "\n",
    "# Get all kmers, in order, with a sliding window of k (but sliding 1bp for each iteration up to k)\n",
    "# Also get RC for all....\n",
    "\n",
    "def kmer_processor(seq,offset):\n",
    "    return list(map(convert_kmer_to_int, get_kmers(k)(seq[offset:])))\n",
    "\n",
    "def get_kmers_from_seq(sequence):\n",
    "    kmers_from_seq = list()\n",
    "\n",
    "    kp = functools.partial(kmer_processor, sequence)\n",
    "    \n",
    "    for i in map(kp, range(0,k)):\n",
    "        kmers_from_seq.append(i)\n",
    "\n",
    "    rev = sequence[::-1]\n",
    "    kpr = functools.partial(kmer_processor, rev)\n",
    "    \n",
    "    for i in map(kpr, range(0,k)):\n",
    "        kmers_from_seq.append(i)\n",
    "            \n",
    "#    for i in range(0,k):\n",
    "#        kmers_from_seq.append(kmer_processor(sequence,i))\n",
    "#    for i in range(0,k):\n",
    "#        kmers_from_seq.append(kmer_processor(rev, i))\n",
    "    return kmers_from_seq\n",
    "\n",
    "data = list()\n",
    "\n",
    "def load_fasta(filename):\n",
    "    data = dict()\n",
    "    file_base_name = ntpath.basename(filename)\n",
    "    picklefilename = file_base_name + \".picklepickle\"\n",
    "    if os.path.isfile(picklefilename):\n",
    "        print(\"Loading from pickle: \" + filename)\n",
    "        data = pickle.load(open(picklefilename, \"rb\"))\n",
    "    else:\n",
    "        print(\"File not found, generating new sequence: \" + picklefilename)\n",
    "        for seq_record in SeqIO.parse(filename, \"fasta\"):\n",
    "            data.update({seq_record.id:\n",
    "                         get_kmers_from_seq(seq_record.seq.upper())})\n",
    "        pickle.dump(data, open(picklefilename, \"wb\"))\n",
    "    return(data)\n",
    "        \n",
    "def get_kmers_from_file(filename):\n",
    "    kmer_list = list()\n",
    "    for seq_record in SeqIO.parse(filename, \"fasta\"):\n",
    "        kmer_list.extend(get_kmers_from_seq(seq_record.seq.upper()))\n",
    "    return set([item for sublist in kmer_list for item in sublist])\n",
    "\n",
    "all_kmers = set()\n",
    "\n",
    "# Very slow, should make this part concurrent...\n",
    "\n",
    "def find_all_kmers(directory):\n",
    "    kmer_master_list = list()\n",
    "    files = [directory + \"/\" + f for f in os.listdir(directory)]\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=4) as executor:\n",
    "        for i in executor.map(get_kmers_from_file, files):\n",
    "            kmer_master_list.extend(list(i))\n",
    "            kmer_master_list = list(set(kmer_master_list))\n",
    "            print(\"Total unique kmers: \" + str(len(set(kmer_master_list))))\n",
    "    return set(kmer_master_list)\n",
    "\n",
    "def get_categories(directory):\n",
    "    data = list()\n",
    "    files = os.listdir(directory)\n",
    "    for filename in files:\n",
    "        for seq_record in SeqIO.parse(directory + \"/\" + filename, \"fasta\"):\n",
    "            data.append(seq_record.id)\n",
    "    data = sorted(list(set(data)))\n",
    "    return(data)\n",
    "\n",
    "def training_file_generator(directory):\n",
    "    files = [directory + \"/\" + f for f in os.listdir(directory)]\n",
    "    random.shuffle(files)\n",
    "    def gen():\n",
    "        nonlocal files\n",
    "        if (len(files) == 0):\n",
    "            files = [directory + \"/\" + f for f in os.listdir(directory)]\n",
    "            random.shuffle(files)\n",
    "        return(files.pop())\n",
    "    return gen\n",
    "\n",
    "def gen_random_training_data(input_data, window_size):\n",
    "    rname = random.choice(list(input_data.keys()))\n",
    "    rdata = random.choice(input_data[rname])\n",
    "    idx = random.randrange(window_size + 1, len(rdata) - window_size - 1)\n",
    "    tdata = list();\n",
    "    for i in range(idx - window_size - 1, idx + window_size):\n",
    "        if (i < 0): continue\n",
    "        if (i >= len(rdata)): break\n",
    "        if type(rdata[idx]) == list: break;\n",
    "        if type(rdata[i]) == list: break\n",
    "        tdata.append(kmer_dict[rdata[i]])\n",
    "    return tdata, rname\n",
    "\n",
    "# The current state is, each training batch is from a single FASTA file (strain, usually)\n",
    "# This can be ok, as long as training batch is a large number\n",
    "# Need to speed up reading of FASTA files though, maybe pyfaidx or something?\n",
    "\n",
    "# Define the one-hot dictionary...\n",
    "\n",
    "replicons_list = get_categories(\"training-files/\")\n",
    "\n",
    "oh = dict()\n",
    "a = 0\n",
    "for i in replicons_list:\n",
    "    oh[i] = tf.one_hot(a, len(replicons_list))\n",
    "    a += 1\n",
    "    \n",
    "oh = dict()\n",
    "a = 0\n",
    "for i in replicons_list:\n",
    "    oh[i] = a\n",
    "    a += 1\n",
    "    \n",
    "oh = dict()\n",
    "oh['Main'] = [1.0, 0.0, 0.0]\n",
    "oh['pSymA'] = [0.0, 1.0, 0.0]\n",
    "oh['pSymB'] = [0.0, 0.0, 1.0]\n",
    "\n",
    "\n",
    "def generate_training_batch(data, batch_size, window_size):\n",
    "    training_batch_data = list();\n",
    "    while len(training_batch_data) < batch_size:\n",
    "         training_batch_data.append(gen_random_training_data(data, \n",
    "                                                             window_size))\n",
    "    return training_batch_data\n",
    "\n",
    "def train_input_fn():\n",
    "    rdata = generate_training_batch(training_data, 1, window_size)[0]\n",
    "    return rdata[0], oh[rdata[1]]\n",
    "    # return {\"train_input\": rdata[0]}, oh[rdata[1]]\n",
    "\n",
    "batch_size = 1024\n",
    "embedding_size = 128\n",
    "window_size = 7\n",
    "\n",
    "replicons_list = get_categories(\"training-files/\")\n",
    "\n",
    "filegen = training_file_generator(\"training-files/\")\n",
    "\n",
    "repdict = dict()\n",
    "a = 0\n",
    "for i in replicons_list:\n",
    "    repdict[i] = a\n",
    "    a += 1\n",
    "\n",
    "def train_input_fn(data):\n",
    "    tbatch = generate_training_batch(data, 1, window_size)\n",
    "    dat = {'x': tf.convert_to_tensor([tf.convert_to_tensor(get_kmer_embeddings(tbatch[0][0]))])}\n",
    "    lab = tf.convert_to_tensor([repdict[tbatch[0][1]]])\n",
    "    return dat, lab\n",
    "\n",
    "def test_input_fn(data):\n",
    "    tbatch = generate_training_batch(data, 1, window_size)\n",
    "    dat = {'x': tf.convert_to_tensor([tf.convert_to_tensor(get_kmer_embeddings(tbatch[0][0]))])}\n",
    "    lab = tf.convert_to_tensor([repdict[tbatch[0][1]]])\n",
    "    return dat, lab\n",
    "\n",
    "def train_input_fn_raw(data):\n",
    "    tbatch = generate_training_batch(data, 1, window_size)\n",
    "    dat = {'x': (get_kmer_embeddings(tbatch[0][0]))}\n",
    "    lab = [repdict[tbatch[0][1]]]\n",
    "    return dat, lab\n",
    "\n",
    "# Because this was run at work on a smaller sample of files....\n",
    "# with open(\"all_kmers_subset.txt\", \"w\") as f:\n",
    "#     for s in all_kmers:\n",
    "#         f.write(str(s) +\"\\n\")\n",
    "\n",
    "sess = tf.Session()\n",
    "\n",
    "# Because this was run at work on a smaller sample of files....\n",
    "all_kmers = list()\n",
    "# with open(\"all_kmers_subset.txt\", \"r\") as f:\n",
    "#     for line in f:\n",
    "#         all_kmers.append(int(line.strip()))\n",
    "\n",
    "all_kmers = pickle.load(open(\"all_kmers.p\", \"rb\"))\n",
    "\n",
    "all_kmers = set(all_kmers)\n",
    "len(all_kmers)\n",
    "# len(data)\n",
    "\n",
    "# all_kmers = set([item for sublist in data for item in sublist])\n",
    "unused_kmers = set(range(0, space)) - all_kmers\n",
    "\n",
    "kmer_dict = dict()\n",
    "reverse_kmer_dict = dict();\n",
    "\n",
    "a = 0\n",
    "for i in all_kmers:\n",
    "    kmer_dict[i] = a\n",
    "    reverse_kmer_dict[a] = i\n",
    "    a += 1\n",
    "    \n",
    "kmer_count = len(all_kmers)\n",
    "\n",
    "[len(all_kmers), len(unused_kmers), space]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This fn now generates all possible combinations of training data....\n",
    "\n",
    "def gen_training_data(input_data, window_size):\n",
    "    total_data = list()\n",
    "    \n",
    "    for k in input_data.keys():\n",
    "        for kdata in input_data[k]:\n",
    "            for i in range(window_size + 1, len(kdata) - window_size):\n",
    "                kentry = list()\n",
    "                for x in range(i - window_size - 1, i + window_size):\n",
    "                    kentry.append(kmer_dict[kdata[x]])\n",
    "                total_data.append([kentry, k])\n",
    "    return total_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "embeddings = np.load(\"final_embeddings.npy\")\n",
    "\n",
    "def get_kmer_embeddings(kmers):\n",
    "    a = list() # numpy.empty(128 * 15)\n",
    "    for k in kmers:\n",
    "        a.append(embeddings[k])\n",
    "    return np.hstack(a)\n",
    "\n",
    "def gen_training_data_generator(input_data, window_size, repdict):\n",
    "    for k in input_data.keys():\n",
    "        for kdata in input_data[k]:\n",
    "            for i in range(window_size + 1, len(kdata) - window_size):\n",
    "                kentry = list()\n",
    "                for x in range(i - window_size - 1, i + window_size):\n",
    "                    kentry.append(kmer_dict[kdata[x]])\n",
    "                yield(get_kmer_embeddings(kentry), [repdict[k]])\n",
    "\n",
    "# Not infinite\n",
    "def kmer_generator(directory, window_size):\n",
    "    files = [directory + \"/\" + f for f in os.listdir(directory)]\n",
    "    random.shuffle(files)\n",
    "    \n",
    "    replicons_list = get_categories(\"training-files/\")\n",
    "    repdict = dict()\n",
    "    a = 0\n",
    "    for i in replicons_list:\n",
    "        repdict[i] = a\n",
    "        a += 1\n",
    "    \n",
    "    for f in files:\n",
    "        yield from gen_training_data_generator(load_fasta(f), window_size, repdict)\n",
    "        \n",
    "# Plan to use tf.data.Dataset.from_generator\n",
    "# ds = tf.contrib.data.Dataset.list_files(\"training-files/\").map(tf_load_fasta)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# next(kmer_gen)\n",
    "\n",
    "\n",
    "\n",
    "# value = ds.make_one_shot_iterator().get_next()\n",
    "# sess.run(value) # It works!\n",
    "\n",
    "# filegen = training_file_generator(\"training-files/\")\n",
    "# training_data = load_fasta(filegen())\n",
    "# a = gen_training_data(training_data, window_size)\n",
    "# len(a)\n",
    "\n",
    "# convert_to_kmer(reverse_kmer_dict[a[0][0][0]])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading from pickle: training-files//M2.final.fasta\n",
      "({'x': array([[  1.13193221e-01,  -2.68094791e-05,  -6.14907704e-02, ...,\n",
      "         -1.60727367e-01,  -8.34798664e-02,  -4.62895632e-02],\n",
      "       [  9.26163420e-02,   1.90128423e-02,  -8.85954127e-02, ...,\n",
      "          1.10865019e-01,  -9.22644064e-02,   3.32754403e-02],\n",
      "       [ -7.84365926e-03,  -8.60286504e-02,   4.05383967e-02, ...,\n",
      "         -3.12674344e-02,  -7.13134855e-02,  -2.20853519e-02],\n",
      "       ..., \n",
      "       [  1.29936472e-01,  -1.29409999e-01,   6.54939041e-02, ...,\n",
      "         -6.82024434e-02,   1.20703876e-01,   1.25639275e-01],\n",
      "       [  4.23520990e-02,   5.45344651e-02,   5.08682169e-02, ...,\n",
      "          7.33557194e-02,  -5.49560040e-02,   7.91435316e-02],\n",
      "       [ -5.54803200e-02,  -8.47043004e-03,  -4.61121388e-02, ...,\n",
      "         -1.29670367e-01,  -1.96228661e-02,  -7.19213262e-02]], dtype=float32)}, array([[1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1]], dtype=int64))\n"
     ]
    }
   ],
   "source": [
    "def my_input_fn():\n",
    "    kmer_gen = functools.partial(kmer_generator, \"training-files/\", window_size)\n",
    "    ds = tf.data.Dataset.from_generator(kmer_gen, (tf.float32, tf.int64)) #, (tf.TensorShape([1920]), tf.TensorShape()))\n",
    "    ds = ds.repeat(1)\n",
    "    ds = ds.shuffle(buffer_size=5000)\n",
    "    ds = ds.batch(1000)\n",
    "    \n",
    "    def add_labels(arr, lab):\n",
    "        return {\"x\": arr}, lab\n",
    "    \n",
    "    ds = ds.map(add_labels)\n",
    "    iterator = ds.make_one_shot_iterator()\n",
    "    batch_features, batch_labels = iterator.get_next()\n",
    "    return batch_features, batch_labels\n",
    "\n",
    "next_batch = my_input_fn()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    first_batch = sess.run(next_batch)\n",
    "print(first_batch)\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_model_dir': 'classifier', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x0000020016D860F0>, '_task_type': 'worker', '_task_id': 0, '_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
     ]
    }
   ],
   "source": [
    "feature_columns = [tf.feature_column.numeric_column(\"x\", shape=[1920])]\n",
    "nn = tf.estimator.DNNClassifier(feature_columns=feature_columns,\n",
    "                                hidden_units = [5000,1000,50],\n",
    "                                activation_fn=tf.nn.relu,\n",
    "                                dropout=0.2,\n",
    "                                model_dir=\"classifier\",\n",
    "                                n_classes=len(replicons_list),\n",
    "                                optimizer=\"Adam\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Create CheckpointSaverHook.\n"
     ]
    }
   ],
   "source": [
    "nn.train(input_fn = my_input_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# tf dataset can make a dataset from a range, then a map fn....\n",
    "# Can feed in a number to gen_training_data to get that to work\n",
    "# tf.contrib.data.Dataset.range(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Can't store kmers as a dataset, as that would just make it much larger to store the data overall\n",
    "# Need to store as tf.data.Iterator compatible fn's\n",
    "\n",
    "# Maybe as a tfRECORD\n",
    "# http://www.machinelearninguru.com/deep_learning/tensorflow/basics/tfrecord/tfrecord.html#create"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embeddings = np.load(\"final_embeddings.npy\")\n",
    "\n",
    "def get_kmer_embeddings(kmers):\n",
    "    a = list() # numpy.empty(128 * 15)\n",
    "    for k in kmers:\n",
    "        a.append(embeddings[k])\n",
    "    return np.hstack(a)\n",
    "\n",
    "# training_data = load_fasta(filegen())\n",
    "# get_kmer_embeddings(tbatch[0][0])\n",
    "\n",
    "# tf.convert_to_tensor([tf.convert_to_tensor(get_kmer_embeddings(tbatch[0][0]))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "WARNING:tensorflow:Using temporary folder as model directory: C:\\Users\\Joey\\AppData\\Local\\Temp\\tmpyxwxw13w\n",
      "INFO:tensorflow:Using config: {'_model_dir': 'C:\\\\Users\\\\Joey\\\\AppData\\\\Local\\\\Temp\\\\tmpyxwxw13w', '_tf_random_seed': 1, '_save_summary_steps': 100, '_save_checkpoints_secs': 600, '_save_checkpoints_steps': None, '_session_config': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100}\n"
     ]
    }
   ],
   "source": [
    "feature_columns = [tf.feature_column.numeric_column(\"x\", shape=[1920])]\n",
    "nn = tf.estimator.DNNClassifier(feature_columns=feature_columns,\n",
    "                                hidden_units = [5000,1000,100,50],\n",
    "                                activation_fn=tf.nn.relu,\n",
    "                                dropout=0.2,\n",
    "                                n_classes=len(replicons_list),\n",
    "                                optimizer=\"Adam\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading from pickle: training-files//SM11.final.fasta\n",
      "Loading from pickle: training-files//KH35c.final.fasta\n",
      "Loading from pickle: training-files//USDA1157.final.fasta\n",
      "Loading from pickle: training-files//HM006.final.fasta\n",
      "Loading from pickle: training-files//Reference.final.fasta\n"
     ]
    }
   ],
   "source": [
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "\n",
    "filegen = training_file_generator(\"training-files/\")\n",
    "\n",
    "for i in xrange(10):\n",
    "    training_data = load_fasta(filegen())\n",
    "    tfn = functools.partial(train_input_fn, training_data)\n",
    "    nn.train(input_fn=tfn, steps=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in xrange(10):\n",
    "    filegen = training_file_generator(\"training-files/\")\n",
    "    training_data = load_fasta(filegen())\n",
    "    tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "    tfn = functools.partial(train_input_fn, training_data)\n",
    "    accuracy_score = nn.evaluate(input_fn=tfn, steps=10000)\n",
    "    print(\"\\nTest Accuracy: {0:f}\\n\".format(accuracy_score['accuracy']))\n",
    "    print(\"Average Loss: {0:f}\".format(accuracy_score['average_loss']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'x': array([-0.01598681,  0.08780892,  0.05043568, ...,  0.03577497,\n",
       "          0.03044195,  0.04390863], dtype=float32)}, [2])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_input_fn_raw(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Emel_M2_Main',\n",
       " 'Emel_M2_Uni1',\n",
       " 'Emel_M2_Uni2',\n",
       " 'Emel_M2_Uni3',\n",
       " 'Emel_WSM419_Main',\n",
       " 'Emel_WSM419_Uni1',\n",
       " 'Emel_WSM419_Uni11',\n",
       " 'Emel_WSM419_Uni2',\n",
       " 'HM006_Accessory_A',\n",
       " 'KH35c_Accessory_A',\n",
       " 'M162_Accessory_A',\n",
       " 'M270_Accessory_A',\n",
       " 'M270_Accessory_B',\n",
       " 'M270_Accessory_C',\n",
       " 'Main',\n",
       " 'Rm41_Accessory_A',\n",
       " 'T073_Accessory_A',\n",
       " 'USDA1021_Accessory_A',\n",
       " 'USDA1157_Accessory_A',\n",
       " 'pHRB800',\n",
       " 'pHRC017',\n",
       " 'pRmeGR4a',\n",
       " 'pRmeGR4b',\n",
       " 'pSINME01',\n",
       " 'pSINME02',\n",
       " 'pSMED03_WSM419',\n",
       " 'pSymA',\n",
       " 'pSymB']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "replicons_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
