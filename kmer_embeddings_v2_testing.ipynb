{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "from six.moves import xrange  # pylint: disable=redefined-builtin\n",
    "from Bio import SeqIO\n",
    "import os\n",
    "import concurrent.futures\n",
    "import functools\n",
    "import math\n",
    "import pickle\n",
    "import sys\n",
    "\n",
    "k = 9\n",
    "\n",
    "\n",
    "space = 5 ** k\n",
    "\n",
    "def partition(n, step, coll):\n",
    "    for i in range(0, len(coll), step):\n",
    "        if (i+n > len(coll)):\n",
    "            break #  raise StopIteration...\n",
    "        yield coll[i:i+n]\n",
    "        \n",
    "def get_kmers(k):\n",
    "    return lambda sequence: partition(k, k, sequence)\n",
    "\n",
    "def convert_nt(c):\n",
    "    return {\"N\": 0, \"A\": 1, \"C\": 2, \"T\": 3, \"G\": 4}.get(c, 0)\n",
    "\n",
    "def convert_nt_complement(c):\n",
    "    return {\"N\": 0, \"A\": 3, \"C\": 4, \"T\": 1, \"G\": 2}.get(c, 0)\n",
    "\n",
    "def convert_kmer_to_int(kmer):\n",
    "    return int(''.join(str(x) for x in (map(convert_nt, kmer))), 5)\n",
    "\n",
    "def convert_kmer_to_int_complement(kmer):\n",
    "    return int(''.join(str(x) for x in reversed(list(map(convert_nt_complement, kmer)))), 5)\n",
    "\n",
    "def convert_base5(n):\n",
    "    return {\"0\": \"N\", \"1\": \"A\", \"2\": \"C\", \"3\": \"T\", \"4\": \"G\"}.get(n,\"N\")\n",
    "\n",
    "def convert_to_kmer(kmer):\n",
    "    return ''.join(map(convert_base5, str(np.base_repr(kmer, 5))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def kmer_processor(seq,offset):\n",
    "    return list(map(convert_kmer_to_int, get_kmers(k)(seq[offset:])))\n",
    "\n",
    "def get_kmers_from_seq(sequence):\n",
    "    kmers_from_seq = list()\n",
    "\n",
    "    kp = functools.partial(kmer_processor, sequence)\n",
    "    \n",
    "    for i in map(kp, range(0,k)):\n",
    "        kmers_from_seq.append(i)\n",
    "\n",
    "    rev = sequence[::-1]\n",
    "    kpr = functools.partial(kmer_processor, rev)\n",
    "    \n",
    "    for i in map(kpr, range(0,k)):\n",
    "        kmers_from_seq.append(i)\n",
    "            \n",
    "#    for i in range(0,k):\n",
    "#        kmers_from_seq.append(kmer_processor(sequence,i))\n",
    "#    for i in range(0,k):\n",
    "#        kmers_from_seq.append(kmer_processor(rev, i))\n",
    "    return kmers_from_seq\n",
    "\n",
    "data = list()\n",
    "\n",
    "def load_fasta(filename):\n",
    "    data = list()\n",
    "    file_base_name = os.path.basename(filename)\n",
    "    picklefilename = file_base_name + \".kmerdict.picklepickle\"\n",
    "    if os.path.isfile(picklefilename):\n",
    "        print(\"Loading from pickle\")\n",
    "        data = pickle.load(open(picklefilename, \"rb\"))\n",
    "    else:\n",
    "        print(\"File not found, generating new sequence: \" + picklefilename)\n",
    "        for seq_record in SeqIO.parse(filename, \"fasta\"):\n",
    "            data.extend(get_kmers_from_seq(seq_record.seq.upper()))\n",
    "        pickle.dump(data, open(picklefilename, \"wb\"))\n",
    "    return(data)\n",
    "        \n",
    "def get_kmers_from_file(filename):\n",
    "    kmer_list = list()\n",
    "    for seq_record in SeqIO.parse(filename, \"fasta\"):\n",
    "        kmer_list.extend(get_kmers_from_seq(seq_record.seq.upper()))\n",
    "    return set([item for sublist in kmer_list for item in sublist])\n",
    "\n",
    "all_kmers = set()\n",
    "\n",
    "# Very slow, should make this part concurrent...\n",
    "\n",
    "def find_all_kmers(directory):\n",
    "    kmer_master_list = list()\n",
    "    files = [directory + \"/\" + f for f in os.listdir(directory)]\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=4) as executor:\n",
    "        for i in executor.map(get_kmers_from_file, files):\n",
    "            kmer_master_list.extend(list(i))\n",
    "            kmer_master_list = list(set(kmer_master_list))\n",
    "            print(\"Total unique kmers: \" + str(len(set(kmer_master_list))))\n",
    "    return set(kmer_master_list)\n",
    "\n",
    "# Because this was run at work on a smaller sample of files....\n",
    "# with open(\"all_kmers_subset.txt\", \"w\") as f:\n",
    "#     for s in all_kmers:\n",
    "#         f.write(str(s) +\"\\n\")\n",
    "\n",
    "# Because this was run at work on a smaller sample of files....\n",
    "all_kmers = list()\n",
    "# with open(\"all_kmers_subset.txt\", \"r\") as f:\n",
    "#     for line in f:\n",
    "#         all_kmers.append(int(line.strip()))\n",
    "\n",
    "# All kmer's in the dataset\n",
    "# Already calculated so not recalculated\n",
    "all_kmers = pickle.load(open(\"all_kmers.p\", \"rb\"))\n",
    "all_kmers = set(all_kmers)\n",
    "\n",
    "kmer_dict = dict()\n",
    "reverse_kmer_dict = dict();\n",
    "\n",
    "a = 0\n",
    "for i in all_kmers:\n",
    "    kmer_dict[i] = a\n",
    "    reverse_kmer_dict[a] = i\n",
    "    a += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def embedding_kmer_generator(directory, window_size):\n",
    "    files = [directory + \"/\" + f for f in os.listdir(directory)]\n",
    "    random.shuffle(files)\n",
    "    \n",
    "    window_range = list(range(-window_size, 0))\n",
    "    window_range.extend(list(range(1, window_size + 1)))\n",
    "\n",
    "    def gen_training_data(idata, window_size):\n",
    "        for data in idata:\n",
    "            for i in xrange(window_size, len(data) + 1 - window_size):\n",
    "                for x in window_range:\n",
    "                    if (x == (i + x)):\n",
    "                        continue\n",
    "                    yield kmer_dict[data[i]], kmer_dict[data[i + x]]\n",
    "    \n",
    "    for f in files:\n",
    "        yield from gen_training_data(load_fasta(f), window_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = embedding_kmer_generator(\"training-files/\", 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def my_input_fn():\n",
    "    kmer_gen = functools.partial(embedding_kmer_generator, \"training-files/\", window_size)\n",
    "\n",
    "    ds = tf.data.Dataset.from_generator(kmer_gen, \n",
    "                                        (tf.int64,\n",
    "                                         tf.int64),\n",
    "                                        (tf.TensorShape(1),\n",
    "                                         tf.TensorShape(None)))\n",
    "                                        \n",
    "    # Numbers reduced to run on my desktop\n",
    "    #ds = ds.repeat(4)\n",
    "    #ds = ds.prefetch(5000000)\n",
    "    #ds = ds.shuffle(buffer_size=500000)\n",
    "    #ds = ds.batch(8000)\n",
    "    \n",
    "    ds = ds.repeat(1)\n",
    "    ds = ds.prefetch(1000)\n",
    "    ds = ds.shuffle(buffer_size=500)\n",
    "    ds = ds.batch(250)\n",
    "    \n",
    "    def add_labels(arr, lab):\n",
    "        return({\"x\": arr}, lab)\n",
    "    \n",
    "    ds = ds.map(add_labels)\n",
    "    iterator = ds.make_one_shot_iterator()\n",
    "    batch_features, batch_labels = iterator.get_next()\n",
    "    return batch_features, batch_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'x': <tf.Tensor 'IteratorGetNext_19:0' shape=(?, 1) dtype=int64>},\n",
       " <tf.Tensor 'IteratorGetNext_19:1' shape=<unknown> dtype=int64>)"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
